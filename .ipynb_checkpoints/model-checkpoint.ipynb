{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入相应的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime  # 用于计算时间\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "#import tensorflow.contrib.keras as kr\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from torchtext import data\n",
    "import jieba\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import tensorwatch as tw\n",
    "import torchvision.models\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(precision=15)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n",
    "np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_path = 'C:/Users/wuxun/Desktop/Data/feat_dat/'\n",
    "image_path = 'C:/Users/wuxun/Desktop/Data/image/'#存储到image文件夹中\n",
    "clip_path = 'C:/Users/wuxun/Desktop/Data/clip/'\n",
    "text_dir = 'C:/Users/wuxun/Desktop/Data/clear_text.txt'\n",
    "vocab_dir = 'C:/Users/wuxun/Desktop/Data/vocab.txt'\n",
    "train_path = 'C:/Users/wuxun/Desktop/Data/training/training.txt'\n",
    "val_path = 'C:/Users/wuxun/Desktop/Data/validation/validation.txt'\n",
    "csv_path = 'D:/csv/'\n",
    "save_path = 'C:/Users/wuxun/Desktop/Data/save_model/'\n",
    "save_path2 = 'C:/Users/wuxun/Desktop/Data/save_model2/params.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#固定随机数种子\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def read_file(filename):\n",
    "\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    \n",
    "    contents = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            contents.append(re.split('[, \\n.]',line))\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(text_dir, vocab_dir, vocab_size=3000):\n",
    "\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train = read_file(text_dir)\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        for k in content:\n",
    "            if len(k)!=0:\n",
    "                all_data.append(k)\n",
    "    print(all_data)\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vocab(vocab_dir):\n",
    "\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "\n",
    "    with open(vocab_dir) as fp:\n",
    "        words = [(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_words(content, words):\n",
    "\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "\n",
    "    return ''.join(words[x] for x in content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dict(path, csv_path):\n",
    "    '''\n",
    "    获得最终的数据集\n",
    "    path:文本数据集\n",
    "    csv_path\n",
    "    '''\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    data_id = []\n",
    "    source_csv=[]\n",
    "    target_csv=[]\n",
    "    fake_time_list=[]\n",
    "    source_time_list=[]\n",
    "    target_time_list=[]\n",
    "    Max_len=-1\n",
    "    count=0\n",
    "    with open(path) as contents:\n",
    "        for line in contents:\n",
    "            count+=1\n",
    "            List = line.split('#')\n",
    "            video_name = List[0]\n",
    "            time_length = float(List[1])\n",
    "            foldtype = List[2]\n",
    "            recipetype = List[3]\n",
    "            source = List[4]\n",
    "            target = List[5]\n",
    "            fake_time = (List[7].split('_'))\n",
    "            fake_time_l=int(fake_time[0])\n",
    "            fake_time_r=int(fake_time[1])\n",
    "            fake_time_list.append([fake_time_l, fake_time_r])\n",
    "            \n",
    "            #将句子转换为id表示：\n",
    "            sentence = List[6].strip('\\n').strip()\n",
    "            sentence = re.split(r\"[,| |.]\",sentence)\n",
    "            sentence_id = [word_to_id[x] for x in sentence if x in word_to_id]\n",
    "            if len(sentence_id) > Max_len:\n",
    "                Max_len = len(sentence_id)\n",
    "            data_id.append(sentence_id)\n",
    "            \n",
    "            #寻找路径,先统一取0001\n",
    "            dir_path = csv_path+'/'+foldtype+'/'+recipetype+'/'+video_name+'/0001/'\n",
    "            name = os.listdir(dir_path)[0]\n",
    "            dir_path = dir_path + name\n",
    "            \n",
    "            #读取csv文件\n",
    "            my_file = Path(dir_path)\n",
    "            if my_file.exists():\n",
    "                frame_sum = pd.read_csv(dir_path, header=None)\n",
    "            else:\n",
    "                print(\"目录不存在！\")\n",
    "            \n",
    "            #确定时间点\n",
    "            source = source.split('_')\n",
    "            target = target.split('_')\n",
    "            \n",
    "            source_time = (float(source[0])+float(source[1]))//2\n",
    "            source_time_list.append([float(source[0]),float(source[1])])\n",
    "            source_frame_num = int(source_time/time_length*500)\n",
    "            source_frame = frame_sum.loc[source_frame_num]\n",
    "            source_csv.append([source_frame])\n",
    "            \n",
    "            target_time = (float(target[0])+float(target[1]))//2\n",
    "            target_time_list.append([float(target[0]),float(target[1])])\n",
    "            target_frame_num = int(target_time/time_length*500)\n",
    "            target_frame = frame_sum.loc[target_frame_num]\n",
    "            target_csv.append([target_frame])\n",
    "            \n",
    "            #添加其余帧数据作为对抗样本\n",
    "            fake_time = (fake_time_l + fake_time_r)//2\n",
    "            fake_time_num = int(fake_time/time_length*500)\n",
    "            #print(str(count)+\"     \"+str(fake_time_num))\n",
    "            fake_frame = frame_sum.loc[fake_time_num]\n",
    "            \n",
    "            #补充数据\n",
    "            source_csv.append([source_frame])\n",
    "            data_id.append(sentence_id)\n",
    "            target_csv.append([fake_frame])\n",
    "            source_time_list.append([float(source[0]),float(source[1])])\n",
    "            target_time_list.append([fake_time_l, fake_time_r])\n",
    "            \n",
    "    #将所有的句子pad为同一最大长度\n",
    "    batch_data_id = np.array([line +[0]*(Max_len-len(line)) \n",
    "                                for line in data_id])\n",
    "    batch_seq = torch.LongTensor(batch_data_id)\n",
    "            \n",
    "    print(len(batch_seq),len(source_csv),len(target_csv))\n",
    "    \n",
    "    return batch_seq, source_csv, target_csv, source_time_list, target_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集\n",
    "x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train = get_dict(train_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证集\n",
    "x_batch_val, x_csv_val, y_csv_val, source_list_val, target_list_val = get_dict(val_path, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lamba = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class LSTM_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(5000, 64)\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        #self.rnn = nn.GRU(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        self.f1 = nn.Sequential(nn.Linear(256,128),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.ReLU())\n",
    "\n",
    "        self.f2 = nn.Sequential(nn.Linear(128,64))\n",
    "        \n",
    "        \n",
    "        self.conv1=torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1,10,3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(2),\n",
    "        )\n",
    "        \n",
    "        self.conv2=torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(10,20,3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(2),\n",
    "        )\n",
    "\n",
    "        #self.fc1=torch.nn.Linear(2520,128)\n",
    "        self.fc1=torch.nn.Linear(512,128)\n",
    "        self.fc1_drop=torch.nn.Dropout(p=0.4)\n",
    "        self.fc2=torch.nn.Linear(128, 64)\n",
    "        #self.fc2_drop=torch.nn.Dropout(p=0.4)\n",
    "        #self.fc3=torch.nn.Linear(50,10)\n",
    "        \n",
    "        #特征融合\n",
    "        self.final_fc = nn.Linear(in_features=128, out_features=64)\n",
    "        self.score_fc = torch.nn.Conv2d(64,3,kernel_size=1,stride=1)\n",
    "        \n",
    "        \n",
    "    def cnnout(self, x2):\n",
    "        in_fc=x2.view(x2.size(0),-1)\n",
    "        out_fc1=self.fc1(in_fc)\n",
    "        out_drop=self.fc1_drop(out_fc1)\n",
    "        out_fc2=self.fc2(out_drop)\n",
    "        return out_fc2\n",
    "        \n",
    "    def forward(self, x1, x2): \n",
    "        if x1.shape[0]!=2:\n",
    "            #lstm\n",
    "            x = self.embedding(x1)\n",
    "            x,_ = self.rnn(x)\n",
    "            x = F.dropout(x,p=0.8)\n",
    "            x = self.f1(x[:,-1,:])\n",
    "            lstm_output = self.f2(x)\n",
    "\n",
    "            #cnn\n",
    "            cnn_out=self.cnnout(x2)\n",
    "            #concat\n",
    "            output = torch.cat((lstm_output, cnn_out), 1)\n",
    "            output = self.final_fc(output)\n",
    "            return output\n",
    "        else:\n",
    "            cnn_out=self.cnnout(x2)\n",
    "            return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(x_batch, x_csv, y_csv, source_list, target_list, batch_size=32):\n",
    "\n",
    "    \"\"\"\n",
    "    生成批次数据\n",
    "    \"\"\"\n",
    "\n",
    "    data_len = x_batch.shape[0]\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_batch_shuffle = x_batch[indices]\n",
    "    x_csv_shuffle =np.array(x_csv)[indices]\n",
    "    y_csv_shuffle = np.array(y_csv)[indices]\n",
    "    source_list = np.array(source_list)[indices]\n",
    "    target_list = np.array(target_list)[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_batch_shuffle[start_id:end_id], x_csv_shuffle[start_id:end_id], y_csv_shuffle[start_id:end_id], source_list[start_id:end_id], target_list[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储loss数据\n",
    "train_loss_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = LSTM_CNN()\n",
    "    #model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/110epoch_20200713_params.pkl'))\n",
    "    Loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "    T=Variable(torch.FloatTensor([[1.0,1.0],[1.0,1.0]]))\n",
    "    best_val_loss = 1000000\n",
    "    print(\"train begin......\")\n",
    "\n",
    "    for epoch in range(100):\n",
    "        batch_train = batch_iter(x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train, batch_size)\n",
    "        print(\"====================  epoch:\"+str(epoch)+\"  ========================\")\n",
    "        train_loss_sum = 0\n",
    "        train_loss_avg = 0\n",
    "        count = 0\n",
    "        for x_batch, x_csv, y_csv, source_time, target_time in batch_train:\n",
    "            if x_csv.shape[0]==batch_size:#保证batch_size\n",
    "                count += 1\n",
    "                x1 = Variable(torch.LongTensor(x_batch))\n",
    "                x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                pred_y = model(x1, x2)\n",
    "                negtive = model(T, x2)\n",
    "                postive = model(T, y)\n",
    "                loss_reg = Loss(pred_y, postive, negtive)\n",
    "                train_loss_sum += loss_reg\n",
    "                optimizer.zero_grad()\n",
    "                loss_reg.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)#梯度裁剪\n",
    "                optimizer.step()\n",
    "                \n",
    "        train_loss_avg = train_loss_sum /count\n",
    "        print(\"train_loss: \"+str(train_loss_sum))\n",
    "        #print(count)\n",
    "        #print(train_loss_avg)\n",
    "        #print(str(train_loss_avg))\n",
    "        train_loss_list.append(train_loss_sum)\n",
    "\n",
    "        #对模型进行验证：\n",
    "        if (epoch+1)%5 == 0:\n",
    "            print(\"进行验证.....\")\n",
    "            count = 0\n",
    "            val_loss_sum = 0\n",
    "            val_loss_avg = 0\n",
    "            batch_val = batch_iter(x_batch_val, x_csv_val, y_csv_val,source_list_val,target_list_val, batch_size)\n",
    "            for x_batch, x_csv, y_csv, source_time, target_time in batch_val:\n",
    "                if x_csv.shape[0]==batch_size:\n",
    "                    count += 1\n",
    "                    x1 = Variable(torch.LongTensor(x_batch))\n",
    "                    x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                    y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                    pred_y = model(x1, x2)\n",
    "                    negtive = model(T, x2)\n",
    "                    postive = model(T, y)\n",
    "                    loss_reg = Loss(pred_y, postive, negtive)\n",
    "                    #loss_reg = torch.abs(pred_y - postive).mean()\n",
    "                    val_loss_sum += loss_reg\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_reg.backward()\n",
    "                    #nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)#梯度裁剪\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            val_loss_avg = val_loss_sum /count\n",
    "            print(\"val_loss: \"+str(val_loss_sum))\n",
    "            val_loss_list.append(val_loss_sum)   \n",
    "            torch.save(model.state_dict(), save_path+str(epoch)+'_not_better_params.pkl')\n",
    "            if val_loss_sum < best_val_loss:\n",
    "                torch.save(model.state_dict(), save_path+str(epoch)+'_better_params.pkl')\n",
    "                best_val_loss = val_loss_sum\n",
    "                print(\"model save!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 绘制Loss曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw(train_loss_list, val_loss_list):\n",
    "    x1 = range(0, len(train_loss_list))\n",
    "    x2 = range(0, len(val_loss_list))\n",
    "    #with plt.style.context(['science']):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x1, train_loss_list[:len(train_loss_list)], 'o-')\n",
    "    plt.title('train loss vs. epoches')\n",
    "    plt.ylabel('train loss')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x2,val_loss_list[:len(val_loss_list)] , '.-')\n",
    "    plt.xlabel('Val loss vs. epoches')\n",
    "    plt.ylabel('Val loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(train_loss_list, val_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#超参数lambda\n",
    "lamba = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class Change(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Change, self).__init__()\n",
    "        #特征融合\n",
    "        self.score_fc = torch.nn.Conv2d(64,3,kernel_size=1,stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        #特征转换\n",
    "        output_expand = output.expand([batch_size, batch_size, 64])\n",
    "        output_expand = output_expand.unsqueeze(0).permute(0,3,1,2)\n",
    "        score = self.score_fc(output_expand).squeeze(0)\n",
    "\n",
    "        alignment_mat = score[0]\n",
    "        l_mat = score[1]\n",
    "        r_mat = score[2]\n",
    "        \n",
    "        I = torch.eye(batch_size)\n",
    "        allone = torch.ones(batch_size, batch_size)\n",
    "        mask = allone-2*I\n",
    "\n",
    "        l_reg = torch.mm(l_mat*I, torch.ones(batch_size,1))\n",
    "        r_reg = torch.mm(r_mat*I, torch.ones(batch_size,1))\n",
    "        offset_pred = torch.cat([l_reg, r_reg],1)\n",
    "        \n",
    "        loss_mat = torch.log(allone + torch.exp(mask * score[0]))\n",
    "        #print(\"======================loss_mat:============================\")\n",
    "        #print((loss_mat.detach().numpy()))\n",
    "        para = I + 1.0 / batch_size * allone\n",
    "        loss_mat = loss_mat * para\n",
    "        loss_alignment = loss_mat.mean()\n",
    "        #print(\"======================loss_alignment:=======================\")\n",
    "        #print((loss_alignment.detach().numpy()))\n",
    "        \n",
    "        return offset_pred, loss_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_2_list=[]\n",
    "val_loss_2_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_2():\n",
    "    change_model = Change()\n",
    "    model = LSTM_CNN()\n",
    "    model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/60poch_20200713_params.pkl'))\n",
    "    optimizer = optim.Adam(change_model.parameters(), lr = 0.001)\n",
    "    best_val_loss_2 = 1000000\n",
    "    print(\"train begin......\")\n",
    " \n",
    "    for epoch in range(200):\n",
    "        batch_train = batch_iter(x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train, batch_size)\n",
    "        print(\"====================  epoch:\"+str(epoch)+\"  ========================\")\n",
    "        count = 0\n",
    "        train_loss_sum = 0\n",
    "        train_loss_avg = 0\n",
    "        train_alignmentloss_sum = 0\n",
    "        train_regloss_sum =0\n",
    "        for x_batch, x_csv, y_csv, source_time, target_time in batch_train:\n",
    "            if x_csv.shape[0]==batch_size:\n",
    "                count += 1\n",
    "                x1 = Variable(torch.LongTensor(x_batch))\n",
    "                x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                source_time = Variable(torch.FloatTensor(np.array(source_time)))\n",
    "                target_time = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "                pred = model(x1, x2)\n",
    "                pred2, loss_alignment = change_model(pred)\n",
    "                loss_reg = torch.abs(pred2 - target_time).mean()\n",
    "                \n",
    "                train_regloss_sum+=loss_reg\n",
    "                loss_reg = loss_reg*lamba + loss_alignment\n",
    "                \n",
    "                train_alignmentloss_sum+=loss_alignment\n",
    "                train_loss_sum +=loss_reg\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_reg.backward()\n",
    "                nn.utils.clip_grad_norm_(change_model.parameters(), max_norm=20, norm_type=2)#梯度裁剪\n",
    "                optimizer.step()\n",
    "                \n",
    "        train_loss_avg = train_loss_sum /count\n",
    "        print('***************'+str(train_loss_avg)+'*************************')\n",
    "        print(\"loss_alignment: \"+str(train_alignmentloss_sum/count))\n",
    "        print(\"loss_reg: \"+str(train_regloss_sum/count))\n",
    "        train_loss_2_list.append(train_loss_avg)\n",
    "        \n",
    "        if (epoch+1)%5==0:\n",
    "            print(\"进行验证.......\")\n",
    "            batch_val = batch_iter(x_batch_val, x_csv_val, y_csv_val,source_list_val,target_list_val, batch_size)\n",
    "            count = 0\n",
    "            val_loss_sum = 0\n",
    "            val_loss_avg = 0 \n",
    "            val_alignmentloss_sum =0\n",
    "            val_regloss_sum =0\n",
    "            for x_batch, x_csv, y_csv, source_time, target_time in batch_val:\n",
    "                if x_csv.shape[0]==batch_size:\n",
    "                    count += 1\n",
    "                    x1 = Variable(torch.LongTensor(x_batch))\n",
    "                    x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                    source_time = Variable(torch.FloatTensor(np.array(source_time)))\n",
    "                    target_time = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "                    pred = model(x1, x2)\n",
    "                    pred2, loss_alignment = change_model(pred)\n",
    "                    \n",
    "                    loss_reg = torch.abs(pred2 - target_time).mean()\n",
    "                    val_regloss_sum +=loss_reg\n",
    "                    loss_reg = loss_reg*lamba + loss_alignment\n",
    "                    \n",
    "                    val_loss_sum +=loss_reg\n",
    "                    val_alignmentloss_sum += loss_alignment\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss_reg.backward()\n",
    "                    nn.utils.clip_grad_norm_(change_model.parameters(), max_norm=20, norm_type=2)#梯度裁剪\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            val_loss_avg = val_loss_sum / count\n",
    "            print(val_loss_avg)\n",
    "            print(\"loss_alignment: \"+str(val_alignmentloss_sum/count))\n",
    "            print(\"loss_reg: \"+str(val_regloss_sum/count))\n",
    "            val_loss_2_list.append(val_loss_avg)        \n",
    "            if val_loss_avg < best_val_loss_2:\n",
    "                torch.save(change_model.state_dict(), save_path2)\n",
    "                best_val_loss_2 = val_loss_avg\n",
    "                print(\"model save!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(train_loss_2_list, val_loss_2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算R@1在tIOU为0.1/0.3/0.5/0.7的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_IoU(i0,i1):\n",
    "    union = (min(i0[0], i1[0]), max(i0[1], i1[1]))\n",
    "    inter = (max(i0[0], i1[0]), min(i0[1], i1[1]))\n",
    "    iou = 1.0*(inter[1]-inter[0])/(union[1]-union[0])\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "compute recall at certain IoU\n",
    "'''\n",
    "def compute_IoU_recall_top_n_forreg(top_n, iou_thresh, sentence_image_mat, sentence_image_reg_mat, sclips, iclips):\n",
    "    correct_num = 0.0\n",
    "    for k in range(sentence_image_mat.shape[0]):\n",
    "        gt = sclips[k]\n",
    "        gt_start = float(gt.split(\"_\")[1])\n",
    "        gt_end = float(gt.split(\"_\")[2])\n",
    "        sim_v = [v for v in sentence_image_mat[k]]\n",
    "        starts = [s for s in sentence_image_reg_mat[k,:,0]]\n",
    "        ends = [e for e in sentence_image_reg_mat[k,:,1]]\n",
    "        picks = nms_temporal(starts,ends, sim_v, iou_thresh-0.05)\n",
    "        #sim_argsort=np.argsort(sim_v)[::-1][0:top_n]\n",
    "        if top_n<len(picks): picks=picks[0:top_n]\n",
    "        for index in picks:\n",
    "            pred_start = sentence_image_reg_mat[k, index, 0]\n",
    "            pred_end = sentence_image_reg_mat[k, index, 1]\n",
    "            iou = calculate_IoU((gt_start, gt_end),(pred_start, pred_end))\n",
    "            if iou>=iou_thresh:\n",
    "                correct_num+=1\n",
    "                break\n",
    "    return correct_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAL_测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = []\n",
    "resualt_text_path='C:/Users/wuxun/Desktop/resulat.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_3():\n",
    "    change_model = Change()\n",
    "    model = LSTM_CNN()\n",
    "    model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/60poch_20200713_params.pkl'))\n",
    "    optimizer = optim.Adam(change_model.parameters(), lr = 0.001)\n",
    "    best_val_loss_2 = 1000000\n",
    "    print(\"train begin......\")\n",
    "    with open(resualt_text_path,'a') as f:\n",
    " \n",
    "        for epoch in range(200):\n",
    "            batch_val = batch_iter(x_batch_val, x_csv_val, y_csv_val,source_list_val,target_list_val, batch_size)\n",
    "            count = 0\n",
    "            val_loss_sum = 0\n",
    "            val_loss_avg = 0 \n",
    "            val_alignmentloss_sum =0\n",
    "            val_regloss_sum =0\n",
    "            for x_batch, x_csv, y_csv, source_time, target_time in batch_val:\n",
    "                if x_csv.shape[0]==batch_size:\n",
    "                    count += 1\n",
    "                    x1 = Variable(torch.LongTensor(x_batch))\n",
    "                    x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                    source_time = Variable(torch.FloatTensor(np.array(source_time)))\n",
    "                    target_time = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "                    pred = model(x1, x2)\n",
    "                    pred2, loss_alignment = change_model(pred)\n",
    "\n",
    "                    loss_reg = torch.abs(pred2 - target_time).mean()\n",
    "                    for i in range(pred2.shape[0]):\n",
    "                        f.write('target_time:[%d, %d]      (l_reg, r_reg):[%.3f, %.3f]\\n' %(target_time[i][0],target_time[i][1],pred2[i][0], pred2[i][1]))\n",
    "                        #print('target_time:[%d, %d]  (l_reg, r_reg):[%.3f, %.3f]' %(target_time[i][0],target_time[i][1],pred2[i][0], pred2[i][1]))\n",
    "                    val_regloss_sum +=loss_reg\n",
    "                    loss_reg = loss_reg*lamba + loss_alignment\n",
    "\n",
    "                    val_loss_sum +=loss_reg\n",
    "                    val_alignmentloss_sum += loss_alignment\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_reg.backward()\n",
    "                    nn.utils.clip_grad_norm_(change_model.parameters(), max_norm=20, norm_type=2)#梯度裁剪\n",
    "                    optimizer.step()\n",
    "\n",
    "            val_loss_avg = val_loss_sum / count\n",
    "            f.write(\"======================================================================================\\n\")\n",
    "            #print(\"============================================================================\")\n",
    "            #print(\"epoch\"+str(epoch)+\": sum_loss:\"+str((val_loss_avg))+\"  loss_alignment: \"+str(val_alignmentloss_sum/count)+\"  loss_reg: \"+str(val_regloss_sum/count))\n",
    "            f.write('[epoch:%d]     sum_loss: %.3f     loss_alignment: %.3f    loss_reg: %.3f\\n' %(epoch, val_loss_avg, val_alignmentloss_sum/count, val_regloss_sum/count))\n",
    "            print('[epoch:%d]   sum_loss: %.3f   loss_alignment: %.3f  loss_reg: %.3f' %(epoch, val_loss_avg, val_alignmentloss_sum/count, val_regloss_sum/count))\n",
    "            val.append(val_loss_avg)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
