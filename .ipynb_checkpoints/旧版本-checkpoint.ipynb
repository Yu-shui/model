{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入相应的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime  # 用于计算时间\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "#import tensorflow.contrib.keras as kr\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from torchtext import data\n",
    "import jieba\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import tensorwatch as tw\n",
    "import torchvision.models\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(precision=15)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import time\n",
    "import matplotlib\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import seaborn as sns\n",
    "# seaborn中文乱码解决方案\n",
    "from matplotlib.font_manager import FontProperties\n",
    "myfont=FontProperties(fname=r'C:\\Windows\\Fonts\\simhei.ttf',size=14)\n",
    "sns.set(font=myfont.get_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_path = 'C:/Users/wuxun/Desktop/Data/feat_dat/'\n",
    "image_path = 'C:/Users/wuxun/Desktop/Data/image/'#存储到image文件夹中\n",
    "clip_path = 'C:/Users/wuxun/Desktop/Data/clip/'\n",
    "text_dir = 'C:/Users/wuxun/Desktop/Data/clear_text.txt'\n",
    "vocab_dir = 'C:/Users/wuxun/Desktop/Data/vocab.txt'\n",
    "train_path = 'C:/Users/wuxun/Desktop/Data/training/training.txt'\n",
    "val_path = 'C:/Users/wuxun/Desktop/Data/validation/validation.txt'\n",
    "csv_path = 'D:/csv/'\n",
    "save_path = 'C:/Users/wuxun/Desktop/Data/save_model/new_model_for_GRU_for_normal_triplet_loss/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#固定随机数种子\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw(train_loss_list, val_loss_list):\n",
    "    x1 = range(0, len(train_loss_list))\n",
    "    x2 = range(0, len(val_loss_list))\n",
    "    #with plt.style.context(['science']):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x1, train_loss_list[:len(train_loss_list)], 'o-')\n",
    "    plt.title('train loss vs. epoches')\n",
    "    plt.ylabel('train loss')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x2,val_loss_list[:len(val_loss_list)] , '.-')\n",
    "    plt.xlabel('Val loss vs. epoches')\n",
    "    plt.ylabel('Val loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0\n",
    "        group['step'] += 1\n",
    "\n",
    "        group['lr'] = args.lr / (1 + group['step'] * args.lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def read_file(filename):\n",
    "\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    \n",
    "    contents = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            contents.append(re.split('[, \\n.]',line))\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(text_dir, vocab_dir, vocab_size=3000):\n",
    "\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train = read_file(text_dir)\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        for k in content:\n",
    "            if len(k)!=0:\n",
    "                all_data.append(k)\n",
    "    print(all_data)\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vocab(vocab_dir):\n",
    "\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "\n",
    "    with open(vocab_dir) as fp:\n",
    "        words = [(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_words(content, words):\n",
    "\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "\n",
    "    return ''.join(words[x] for x in content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dict(path, csv_path):\n",
    "    '''\n",
    "    获得最终的数据集\n",
    "    path:文本数据集\n",
    "    csv_path\n",
    "    '''\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    data_id = []\n",
    "    source_csv=[]\n",
    "    target_csv=[]\n",
    "    fake_time_list=[]\n",
    "    source_time_list=[]\n",
    "    target_time_list=[]\n",
    "    Max_len=-1\n",
    "    count=0\n",
    "    with open(path) as contents:\n",
    "        for line in contents:\n",
    "            count+=1\n",
    "            List = line.split('#')\n",
    "            video_name = List[0]\n",
    "            time_length = float(List[1])\n",
    "            foldtype = List[2]\n",
    "            recipetype = List[3]\n",
    "            source = List[4]\n",
    "            target = List[5]\n",
    "            fake_time = (List[7].split('_'))\n",
    "            fake_time_l=int(fake_time[0])\n",
    "            fake_time_r=int(fake_time[1])\n",
    "            fake_time_list.append([fake_time_l, fake_time_r])\n",
    "            \n",
    "            #将句子转换为id表示：\n",
    "            sentence = List[6].strip('\\n').strip()\n",
    "            sentence = re.split(r\"[,| |.]\",sentence)\n",
    "            sentence_id = [word_to_id[x] for x in sentence if x in word_to_id]\n",
    "            if len(sentence_id) > Max_len:\n",
    "                Max_len = len(sentence_id)\n",
    "            data_id.append(sentence_id)\n",
    "            \n",
    "            #寻找路径,先统一取0001\n",
    "            dir_path = csv_path+'/'+foldtype+'/'+recipetype+'/'+video_name+'/0001/'\n",
    "            name = os.listdir(dir_path)[0]\n",
    "            dir_path = dir_path + name\n",
    "            \n",
    "            #读取csv文件\n",
    "            my_file = Path(dir_path)\n",
    "            if my_file.exists():\n",
    "                frame_sum = pd.read_csv(dir_path, header=None)\n",
    "            else:\n",
    "                print(\"目录不存在！\")\n",
    "            \n",
    "            #确定时间点\n",
    "            source = source.split('_')\n",
    "            target = target.split('_')\n",
    "            \n",
    "            source_time = (float(source[0])+float(source[1]))//2\n",
    "            source_time_list.append([float(source[0]),float(source[1])])\n",
    "            source_frame_num = int(source_time/time_length*500)\n",
    "            source_frame = frame_sum.loc[source_frame_num]\n",
    "            source_csv.append([source_frame])\n",
    "            \n",
    "            target_time = (float(target[0])+float(target[1]))//2\n",
    "            target_time_list.append([float(target[0]),float(target[1])])\n",
    "            target_frame_num = int(target_time/time_length*500)\n",
    "            target_frame = frame_sum.loc[target_frame_num]\n",
    "            target_csv.append([target_frame])\n",
    "            \n",
    "            '''\n",
    "            gt_dist:中间值与长度的欧氏距离之和\n",
    "            '''\n",
    "\n",
    "            \n",
    "            \n",
    "    #将所有的句子pad为同一最大长度\n",
    "    batch_data_id = np.array([line +[0]*(Max_len-len(line)) \n",
    "                                for line in data_id])\n",
    "    batch_seq = torch.LongTensor(batch_data_id)\n",
    "            \n",
    "    print(len(batch_seq),len(source_csv),len(target_csv))\n",
    "    \n",
    "    return batch_seq, source_csv, target_csv, source_time_list, target_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d40d03f1e151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#训练集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_csv_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_csv_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_list_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-7d4968df3c74>\u001b[0m in \u001b[0;36mget_dict\u001b[1;34m(path, csv_path)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m#寻找路径,先统一取0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mdir_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfoldtype\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecipetype\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/0001/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mdir_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdir_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练集\n",
    "x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train = get_dict(train_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523 523 523\n"
     ]
    }
   ],
   "source": [
    "#验证集\n",
    "x_batch_val, x_csv_val, y_csv_val, source_list_val, target_list_val = get_dict(val_path, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_data(data_list):\n",
    "    print(type(data_list[0]))\n",
    "    print(len(data_list))\n",
    "    Max_v=-1;\n",
    "    Min_v=2000\n",
    "    for i in range(len(data_list)):\n",
    "        if data_list[i] > Max_v:\n",
    "            Max_v = data_list[i]\n",
    "        if data_list[i] < Min_v:\n",
    "            Min_v = data_list[i]\n",
    "    print(Max_v, Min_v)\n",
    "    num_block = int((Max_v - Min_v)/10)\n",
    "    print(num_block)\n",
    "    plt.hist(x = data_list, # 指定绘图数据\n",
    "         bins = num_block + 1, # 指定直方图中条块的个数\n",
    "         color = 'steelblue', # 指定直方图的填充色\n",
    "         edgecolor = 'black' # 指定直方图的边框色\n",
    "         )\n",
    "    # 添加x轴和y轴标签\n",
    "    plt.xlabel('偏差')\n",
    "    plt.ylabel('频数')\n",
    "    # 添加标题\n",
    "    plt.title('偏差分布')\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, mean=0, std=0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        其实这就是一个裁剪的模块，裁剪多出来的padding\n",
    "        \"\"\"\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        \"\"\"\n",
    "        相当于一个Residual block\n",
    "\n",
    "        :param n_inputs: int, 输入通道数\n",
    "        :param n_outputs: int, 输出通道数\n",
    "        :param kernel_size: int, 卷积核尺寸\n",
    "        :param stride: int, 步长，一般为1\n",
    "        :param dilation: int, 膨胀系数\n",
    "        :param padding: int, 填充系数\n",
    "        :param dropout: float, dropout比率\n",
    "        \"\"\"\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        # 经过conv1，输出的size其实是(Batch, input_channel, seq_len + padding)\n",
    "        self.chomp1 = Chomp1d(padding)  # 裁剪掉多出来的padding部分，维持输出时间步为seq_len\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)  #  裁剪掉多出来的padding部分，维持输出时间步为seq_len\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        参数初始化\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: size of (Batch, input_channel, seq_len)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        \"\"\"\n",
    "        TCN，目前paper给出的TCN结构很好的支持每个时刻为一个数的情况，即sequence结构，\n",
    "        对于每个时刻为一个向量这种一维结构，勉强可以把向量拆成若干该时刻的输入通道，\n",
    "        对于每个时刻为一个矩阵或更高维图像的情况，就不太好办。\n",
    "\n",
    "        :param num_inputs: int， 输入通道数\n",
    "        :param num_channels: list，每层的hidden_channel数，例如[25,25,25,25]表示有4个隐层，每层hidden_channel数为25\n",
    "        :param kernel_size: int, 卷积核尺寸\n",
    "        :param dropout: float, drop_out比率\n",
    "        \"\"\"\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            '''\n",
    "            :512, [256, ]\n",
    "            '''\n",
    "            dilation_size = 2 ** i   # 膨胀系数：1，2，4，8……\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]  # 确定每一层的输入通道数\n",
    "            out_channels = num_channels[i]  # 确定每一层的输出通道数\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入x的结构不同于RNN，一般RNN的size为(Batch, seq_len, channels)或者(seq_len, Batch, channels)，\n",
    "        这里把seq_len放在channels后面，把所有时间步的数据拼起来，当做Conv1d的输入尺寸，实现卷积跨时间步的操作，\n",
    "        很巧妙的设计。\n",
    "        \n",
    "        :param x: size of (Batch, input_channel, seq_len)\n",
    "        :return: size of (Batch, output_channel, seq_len)\n",
    "        \"\"\"\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class LSTM_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(5000, 64)\n",
    "        self.rnn = nn.GRU(input_size=64, hidden_size=256, num_layers=2, bidirectional=True)\n",
    "        self.f1 = nn.Sequential(nn.Linear(512,128), nn.Dropout(0.8), nn.ReLU())        \n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(512, 256, kernel_size=1, stride=1)\n",
    "        self.conv2 = torch.nn.Conv1d(256, 128, kernel_size=1, stride=1)\n",
    "\n",
    "        self.fc1=torch.nn.Linear(512,128)\n",
    "        self.fc1_drop=torch.nn.Dropout(p=0.4)\n",
    "        self.fc2=torch.nn.Linear(128, 64)\n",
    "        \n",
    "        #特征融合\n",
    "        self.final_fc = nn.Linear(in_features=256, out_features=128)\n",
    "        # Initializing weights\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "        \n",
    "    def cnnout(self, x2):\n",
    "        x2 = x2.permute(0, 2, 1)\n",
    "        out1 = self.conv1(x2)\n",
    "        #out1_norm = F.normalize(out1, p=2, dim = 1)\n",
    "        out2 = self.conv2(out1_norm)\n",
    "        #out2_norm = F.normalize(out2, p=2, dim = 1)\n",
    "        return out2_norm\n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2): \n",
    "        if x1.shape[0]!=2:\n",
    "            #lstm\n",
    "            x = self.embedding(x1)\n",
    "            x,_ = self.rnn(x)\n",
    "            x = F.dropout(x, p=0.8)\n",
    "            lstm_output = self.f1(x[:,-1,:])\n",
    "            #cnn\n",
    "            cnn_out=self.cnnout(x2)\n",
    "            #concat\n",
    "            cnn_out = cnn_out.squeeze(2)\n",
    "            output = torch.cat((lstm_output, cnn_out), 1)\n",
    "            output = self.final_fc(output)\n",
    "            return output\n",
    "        else:\n",
    "            cnn_out=self.cnnout(x2)\n",
    "            cnn_out = cnn_out.squeeze(2)\n",
    "            return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 定义LSTM的结构\n",
    "# class LSTM_CNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(LSTM_CNN, self).__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(5000, 64)\n",
    "#         self.rnn = nn.LSTM(input_size=64, hidden_size=256, num_layers=2, bidirectional=True)\n",
    "#         self.f1 = nn.Sequential(nn.Linear(512,128), nn.Dropout(0.8), nn.ReLU())        \n",
    "        \n",
    "#         self.conv1 = torch.nn.Conv1d(512, 256, kernel_size=1, stride=1)\n",
    "#         self.conv2 = torch.nn.Conv1d(256, 128, kernel_size=1, stride=1)\n",
    "\n",
    "#         self.fc1=torch.nn.Linear(512,128)\n",
    "#         self.fc1_drop=torch.nn.Dropout(p=0.4)\n",
    "#         self.fc2=torch.nn.Linear(128, 64)\n",
    "        \n",
    "#         #特征融合\n",
    "#         self.final_fc = nn.Linear(in_features=256, out_features=128)\n",
    "#         # Initializing weights\n",
    "#         self.apply(weights_init)\n",
    "        \n",
    "        \n",
    "#     def cnnout(self, x2):\n",
    "#         x2 = x2.permute(0, 2, 1)\n",
    "#         out1 = self.conv1(x2)\n",
    "#         out1_norm = F.normalize(out1, p=2, dim = 1)\n",
    "#         out2 = self.conv2(out1_norm)\n",
    "#         out2_norm = F.normalize(out2, p=2, dim = 1)\n",
    "#         return out2_norm\n",
    "        \n",
    "        \n",
    "#     def forward(self, x1, x2): \n",
    "#         if x1.shape[0]!=2:\n",
    "#             #lstm\n",
    "#             x = self.embedding(x1)\n",
    "#             x,_ = self.rnn(x)\n",
    "#             x = F.dropout(x, p=0.8)\n",
    "#             lstm_output = self.f1(x[:,-1,:])\n",
    "#             #cnn\n",
    "#             cnn_out=self.cnnout(x2)\n",
    "#             #concat\n",
    "#             cnn_out = cnn_out.squeeze(2)\n",
    "#             output = torch.cat((lstm_output, cnn_out), 1)\n",
    "#             output = self.final_fc(output)\n",
    "#             return output\n",
    "#         else:\n",
    "#             cnn_out=self.cnnout(x2)\n",
    "#             cnn_out = cnn_out.squeeze(2)\n",
    "#             return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 定义LSTM的结构\n",
    "# class LSTM_for_two_CNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(LSTM_for_two_CNN, self).__init__()\n",
    "        \n",
    "#         '''\n",
    "#         语义特征提取\n",
    "#         '''\n",
    "#         self.embedding = nn.Embedding(5000, 64)\n",
    "#         self.rnn = nn.LSTM(input_size=64, hidden_size=256, num_layers=2, bidirectional=True)\n",
    "#         self.f1 = nn.Sequential(nn.Linear(512,128), nn.Dropout(0.8), nn.ReLU())     \n",
    "        \n",
    "#         '''\n",
    "#         图像特征提取（混合）\n",
    "#         '''\n",
    "#         self.conv1 = torch.nn.Conv1d(512, 256, kernel_size=1, stride=1)\n",
    "#         self.conv2 = torch.nn.Conv1d(256, 128, kernel_size=1, stride=1)\n",
    "        \n",
    "#         '''\n",
    "#         图像特征提取（单独模型）\n",
    "#         '''\n",
    "#         self.conv3 = torch.nn.Conv1d(512, 256, kernel_size=1, stride=1)\n",
    "#         self.conv4 = torch.nn.Conv1d(256, 128, kernel_size=1, stride=1)\n",
    "        \n",
    "\n",
    "#         self.fc1=torch.nn.Linear(512,128)\n",
    "#         self.fc1_drop=torch.nn.Dropout(p=0.4)\n",
    "#         self.fc2=torch.nn.Linear(128, 64)\n",
    "        \n",
    "#         '''\n",
    "#         特征融合\n",
    "#         '''\n",
    "#         self.final_fc = nn.Linear(in_features=256, out_features=128)\n",
    "        \n",
    "#         # Initializing weights\n",
    "#         self.apply(weights_init)\n",
    "        \n",
    "        \n",
    "#     def cnnout_for_mixture(self, x2):\n",
    "#         x2 = x2.permute(0, 2, 1)\n",
    "#         out1 = self.conv1(x2)\n",
    "#         out1_norm = F.normalize(out1, p=2, dim = 1)\n",
    "#         out2 = self.conv2(out1_norm)\n",
    "#         out2_norm = F.normalize(out2, p=2, dim = 1)\n",
    "#         return out2_norm\n",
    "    \n",
    "#     def cnnout_for_simple(self, x):\n",
    "#         x2 = x.permute(0, 2, 1)\n",
    "#         out3 = self.conv3(x2)\n",
    "#         out3_norm = F.normalize(out3, p=2, dim = 1)\n",
    "#         out4 = self.conv4(out3_norm)\n",
    "#         out4_norm = F.normalize(out4, p=2, dim = 1)\n",
    "#         return out4_norm\n",
    "        \n",
    "        \n",
    "#     def forward(self, x1, x2, x3): \n",
    "#         '''\n",
    "#         提取语义特征\n",
    "#         '''\n",
    "#         x = self.embedding(x1)\n",
    "#         x, _ = self.rnn(x)\n",
    "#         x = F.dropout(x, p=0.8)\n",
    "#         lstm_output = self.f1(x[:,-1,:])\n",
    "        \n",
    "#         '''\n",
    "#         source图像特征\n",
    "#         '''\n",
    "#         cnn_out = self.cnnout_for_mixture(x2)\n",
    "#         cnn_out = cnn_out.squeeze(2)\n",
    "\n",
    "#         source_feature = torch.cat((lstm_output, cnn_out), 1)\n",
    "#         source_feature = self.final_fc(source_feature)\n",
    "        \n",
    "#         '''\n",
    "#         target图像特征\n",
    "#         '''\n",
    "#         target_feature = self.cnnout_for_simple(x3)\n",
    "#         target_feature = target_feature.squeeze(2)\n",
    "        \n",
    "#         return source_feature, target_feature, cnn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogRatioLoss 损失函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRatioLoss(Function):\n",
    "    \"\"\"Log ratio loss function. \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LogRatioLoss, self).__init__()\n",
    "        self.pdist = L2dist(2)  # norm 2\n",
    "\n",
    "    def forward(self, input, gt_dist):\n",
    "        m = input.size()[0]-1   # #paired\n",
    "        a = input[0]            # anchor\n",
    "        p = input[1:]           # paired\n",
    "        \n",
    "        # auxiliary variables\n",
    "        idxs = torch.arange(1, m+1)\n",
    "        indc = idxs.repeat(m,1).t() < idxs.repeat(m,1)\n",
    "\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        dist = self.pdist.forward(a,p)\n",
    "\n",
    "        log_dist = torch.log(dist + epsilon)\n",
    "        log_gt_dist = torch.log(gt_dist + epsilon)\n",
    "        diff_log_dist = log_dist.repeat(m,1).t()-log_dist.repeat(m, 1)\n",
    "        diff_log_gt_dist = log_gt_dist.repeat(m,1).t()-log_gt_dist.repeat(m, 1)\n",
    "\n",
    "        # uniform weight coefficients \n",
    "        wgt = indc.clone().float()\n",
    "        wgt = wgt.div(wgt.sum())\n",
    "\n",
    "        log_ratio_loss = (diff_log_dist-diff_log_gt_dist).pow(2)\n",
    "\n",
    "        loss = log_ratio_loss\n",
    "        loss = loss.mul(wgt).sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成批次数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_resualt_file(path, line):\n",
    "    with open(path, 'a') as f:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(x_batch, x_csv, y_csv, source_list, target_list, batch_size=32):\n",
    "\n",
    "    \"\"\"\n",
    "    生成批次数据\n",
    "    \"\"\"\n",
    "\n",
    "    data_len = x_batch.shape[0]\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_batch_shuffle = x_batch[indices]\n",
    "    x_csv_shuffle =np.array(x_csv)[indices]\n",
    "    y_csv_shuffle = np.array(y_csv)[indices]\n",
    "    source_list = np.array(source_list)[indices]\n",
    "    target_list = np.array(target_list)[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_batch_shuffle[start_id:end_id], x_csv_shuffle[start_id:end_id], y_csv_shuffle[start_id:end_id], source_list[start_id:end_id], target_list[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储loss数据\n",
    "train_loss_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inital_epoch, save_path, batch_size):\n",
    "    model = LSTM_CNN()\n",
    "    #model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/new_model_for_1500/529_params.pkl'))\n",
    "    Loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "    #Loss = LogRatioLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "    T=Variable(torch.FloatTensor([[1.0,1.0],[1.0,1.0]]))\n",
    "    best_val_loss = 1000000\n",
    "    print(\"train begin......\")\n",
    "    \n",
    "    for epoch in range(300):\n",
    "        batch_train = batch_iter(x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train, batch_size)\n",
    "        train_loss_sum = 0\n",
    "        train_loss_avg = 0\n",
    "        count = 0\n",
    "        begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        for x_batch, x_csv, y_csv, source_time, target_time in batch_train:\n",
    "            if x_csv.shape[0]==batch_size:\n",
    "                count += 1\n",
    "                x1 = Variable(torch.LongTensor(x_batch))\n",
    "                x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                pred_y = model(x1, x2)\n",
    "                negtive = model(T, x2)\n",
    "                postive = model(T, y)\n",
    "                loss_reg = Loss(pred_y, postive, negtive)\n",
    "                train_loss_sum += loss_reg\n",
    "                optimizer.zero_grad()\n",
    "                loss_reg.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        train_loss_avg = train_loss_sum / count\n",
    "        line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.6f |' % (inital_epoch + epoch, train_loss_avg))\n",
    "        print(line)\n",
    "        write_resualt_file(save_path + 'resualt.txt', line)\n",
    "        train_loss_list.append(train_loss_sum)\n",
    "\n",
    "        if (epoch+1)%5 == 0:\n",
    "            print(\"进行验证.....\")\n",
    "            begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            count = 0\n",
    "            val_loss_sum = 0\n",
    "            val_loss_avg = 0\n",
    "            batch_val = batch_iter(x_batch_val, x_csv_val, y_csv_val,source_list_val,target_list_val, batch_size)\n",
    "            for x_batch, x_csv, y_csv, source_time, target_time in batch_val:\n",
    "                if x_csv.shape[0]==batch_size:\n",
    "                    count += 1\n",
    "                    x1 = Variable(torch.LongTensor(x_batch))\n",
    "                    x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                    y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                    pred_y = model(x1, x2)\n",
    "                    negtive = model(T, x2)\n",
    "                    postive = model(T, y)\n",
    "                    loss_reg = Loss(pred_y, postive, negtive)\n",
    "                    val_loss_sum += loss_reg\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_reg.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            val_loss_avg = val_loss_sum / count\n",
    "            line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.6f |' % (inital_epoch + epoch, val_loss_avg))\n",
    "            print(line)\n",
    "            write_resualt_file(save_path + 'resualt.txt', line)\n",
    "            val_loss_list.append(val_loss_sum)   \n",
    "            torch.save(model.state_dict(), save_path+str(inital_epoch + epoch)+'_params.pkl')\n",
    "            if val_loss_sum < best_val_loss:\n",
    "                #torch.save(model.state_dict(), save_path+str(inital_epoch + epoch)+'_params.pkl')\n",
    "                best_val_loss = val_loss_sum\n",
    "                print(\"model save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train_for_two_CNN(inital_epoch):\n",
    "#     model = LSTM_for_two_CNN()\n",
    "#     model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/model_for_1500_for_two_CNN_normal_tripletloss/39_params.pkl'))\n",
    "#     Loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 0.005)\n",
    "#     best_val_loss = 1000000\n",
    "#     print(\"train begin......\")\n",
    "    \n",
    "#     for epoch in range(300):\n",
    "#         batch_train = batch_iter(x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train, batch_size)\n",
    "#         train_loss_sum = 0\n",
    "#         train_loss_avg = 0\n",
    "#         count = 0\n",
    "#         begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "#         for x_batch, x_csv, y_csv, source_time, target_time in batch_train:\n",
    "#             if x_csv.shape[0]==batch_size:\n",
    "#                 count += 1\n",
    "#                 x1 = Variable(torch.LongTensor(x_batch))\n",
    "#                 x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "#                 y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "#                 source_feature, target_feature, negtive = model(x1, x2, y)\n",
    "#                 loss_reg = Loss(source_feature, target_feature, negtive)\n",
    "#                 train_loss_sum += loss_reg\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss_reg.backward()\n",
    "#                 optimizer.step()\n",
    "        \n",
    "#         current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "#         train_loss_avg = train_loss_sum / count\n",
    "#         line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.9f |' % (inital_epoch + epoch, train_loss_avg))\n",
    "#         print(line)\n",
    "#         write_resualt_file(save_path+'resualt.txt', line)\n",
    "#         train_loss_list.append(train_loss_sum)\n",
    "\n",
    "#         if (epoch+1)%5 == 0:\n",
    "#             print(\"进行验证.....\")\n",
    "#             begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "#             count = 0\n",
    "#             val_loss_sum = 0\n",
    "#             val_loss_avg = 0\n",
    "#             batch_val = batch_iter(x_batch_val, x_csv_val, y_csv_val,source_list_val,target_list_val, batch_size)\n",
    "#             for x_batch, x_csv, y_csv, source_time, target_time in batch_val:\n",
    "#                 if x_csv.shape[0]==batch_size:\n",
    "#                     count += 1\n",
    "#                     x1 = Variable(torch.LongTensor(x_batch))\n",
    "#                     x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "#                     y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "#                     source_feature, target_feature, negtive = model(x1, x2, y)\n",
    "#                     loss_reg = Loss(source_feature, target_feature, negtive)\n",
    "#                     val_loss_sum += loss_reg\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss_reg.backward()\n",
    "#                     optimizer.step()\n",
    "            \n",
    "#             current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "#             val_loss_avg = val_loss_sum / count\n",
    "#             line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.9f |' % (inital_epoch + epoch, val_loss_avg))\n",
    "#             print(line)\n",
    "#             write_resualt_file(save_path+'resualt.txt', line)\n",
    "#             val_loss_list.append(val_loss_sum)   \n",
    "#             torch.save(model.state_dict(), save_path+str(inital_epoch + epoch)+'_params.pkl')\n",
    "#             if val_loss_sum < best_val_loss:\n",
    "#                 #torch.save(model.state_dict(), save_path+str(inital_epoch + epoch)+'_params.pkl')\n",
    "#                 best_val_loss = val_loss_sum\n",
    "#                 print(\"model save!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begin......\n"
     ]
    }
   ],
   "source": [
    "train(0, save_path, batch_size =128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_for_two_CNN(39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 绘制Loss曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAERCAYAAABsNEDqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRGZh4iAiIrMwcRARkVmYOIiIyCz/HxxDTIuleNLRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(train_loss_list, val_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#超参\n",
    "lamba = 0.01\n",
    "batch_size2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class Change(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Change, self).__init__()\n",
    "        self.score_fc = torch.nn.Conv2d(128, 3, kernel_size=1, stride=1)\n",
    "        #self.apply(weights_init)\n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        #特征转换\n",
    "        output_expand = output.expand([batch_size2, batch_size2, 128])\n",
    "        output_expand = output_expand.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        score = self.score_fc(output_expand).squeeze(0)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归模型二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 定义LSTM的结构\n",
    "# class Change(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Change, self).__init__()\n",
    "#         #特征融合\n",
    "#         self.score_fc = torch.nn.Conv1d(128, 2, kernel_size=1, stride=1)\n",
    "# #         self.score_fc2 = torch.nn.Conv1d(64, 2, kernel_size=1, stride=1)\n",
    "#         self.apply(weights_init)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output = x#[batch_size2, 128]\n",
    "#         output = output.unsqueeze(2)\n",
    "#         score = self.score_fc(output)\n",
    "# #         score = self.score_fc2(score)\n",
    "#         offset_pred = score.squeeze(2)\n",
    "#         return offset_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 路径定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path2 = 'C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2无alignment/'\n",
    "save_resualt_path2 = 'C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2无alignment/resualt.txt'\n",
    "save_path2_with_align = 'C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2有alignment/'\n",
    "save_resualt_path2_with_align = 'C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2有alignment/resualt.txt'\n",
    "save_path_for_two_CNN = 'C:/Users/wuxun/Desktop/Data/save_model2/two_CNN_model_有alignment/'\n",
    "save_path_for_two_CNN_resualt = 'C:/Users/wuxun/Desktop/Data/save_model2/two_CNN_model_有alignment/resualt.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_2_list=[]\n",
    "val_loss_2_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train_for_Change_model_3(resualt_path, model_save_path, inital_epoch):\n",
    "#     change_model = Change()\n",
    "#     model = LSTM_CNN()\n",
    "#     model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/new_model_for_1500/494_params.pkl'))\n",
    "    \n",
    "#     #冻结基础模型参数\n",
    "#     model.eval()\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "#     #change_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2无alignment/batchsize_is_1_19.pkl'))\n",
    "#     optimizer = optim.Adam(change_model.parameters(), lr = 0.001)\n",
    "#     best_val_loss_2 = 1000000\n",
    "#     print(\"train begin......\")\n",
    "#     for epoch in range(1000):\n",
    "#         batch_train = batch_iter(x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train, batch_size2)\n",
    "#         count = 0\n",
    "#         train_loss_sum = 0\n",
    "#         begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "#         for x_batch, x_csv, y_csv, source_time, target_time in batch_train:\n",
    "#             if x_csv.shape[0]==batch_size2:\n",
    "#                 count += 1\n",
    "#                 x1 = Variable(torch.LongTensor(x_batch))\n",
    "#                 x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "#                 source_time = Variable(torch.FloatTensor(np.array(source_time)))\n",
    "#                 target_time = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "#                 pred = model(x1, x2)\n",
    "#                 pred2 = change_model(pred)\n",
    "            \n",
    "#                 Loss = torch.abs(pred2 - target_time).mean()\n",
    "                \n",
    "#                 train_loss_sum += Loss\n",
    "#                 optimizer.zero_grad()\n",
    "#                 Loss.backward()\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#         current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())        \n",
    "#         line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.6f |' % (inital_epoch + epoch, train_loss_sum / (count)))\n",
    "#         print(line)\n",
    "#         write_resualt_file(resualt_path, line)\n",
    "#         train_loss_2_list.append(train_loss_sum / count)\n",
    "\n",
    "#         if (epoch+1)%5==0:\n",
    "#             print(\"进行验证.......\")\n",
    "#             begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "#             batch_val = batch_iter(x_batch_val, x_csv_val, y_csv_val,source_list_val,target_list_val, batch_size2)\n",
    "#             count = 0\n",
    "#             val_loss_sum = 0\n",
    "#             for x_batch, x_csv, y_csv, source_time, target_time in batch_val:\n",
    "#                 if x_csv.shape[0]==batch_size2:\n",
    "#                     count += 1\n",
    "#                     x1 = Variable(torch.LongTensor(x_batch))\n",
    "#                     x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "#                     source_time = Variable(torch.FloatTensor(np.array(source_time)))\n",
    "#                     target_time = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "#                     pred = model(x1, x2)\n",
    "#                     pred2 = change_model(pred)\n",
    "                    \n",
    "#                     Loss = torch.abs(pred2 - target_time).mean()\n",
    "\n",
    "#                     val_loss_sum += Loss\n",
    "#                     optimizer.zero_grad()\n",
    "#                     Loss.backward()\n",
    "#                     optimizer.step()\n",
    "            \n",
    "#             current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "#             line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.6f |' % (inital_epoch + epoch, val_loss_sum / (count)))\n",
    "#             print(line)\n",
    "#             write_resualt_file(resualt_path, line)\n",
    "#             val_loss_2_list.append(val_loss_sum / count) \n",
    "#             #torch.save(change_model.state_dict(), model_save_path + 'batchsize_is_64_the_first_model_300_' + str(inital_epoch + epoch)+'.pkl')\n",
    "#             if (val_loss_sum / count ) < best_val_loss_2:\n",
    "#                 best_val_loss_2 = val_loss_sum / count\n",
    "#                 print(\"model save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_2(resualt_path, model_save_path, inital_epoch):\n",
    "    change_model = Change()\n",
    "    model = LSTM_for_two_CNN()\n",
    "    model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/model_for_1500_for_two_CNN_normal_tripletloss/58_params.pkl'))\n",
    "    \n",
    "    #冻结基础模型参数\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    #change_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2有alignment/epoch_144.pkl'))\n",
    "    optimizer = optim.Adam(change_model.parameters(), lr = 0.001)#0.001\n",
    "    best_val_loss_2 = 1000000\n",
    "    print(\"train begin......\")\n",
    "    for epoch in range(1000):\n",
    "        batch_train = batch_iter(x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train, batch_size2)\n",
    "        count = 0\n",
    "        train_loss_sum = 0\n",
    "        train_alignmentloss_sum = 0\n",
    "        train_regloss_sum =0\n",
    "        begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        for x_batch, x_csv, y_csv, source_time, target_time in batch_train:\n",
    "            if x_csv.shape[0]==batch_size2:\n",
    "                count += 1\n",
    "                x1 = Variable(torch.LongTensor(x_batch))\n",
    "                x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                source_time = Variable(torch.FloatTensor(np.array(source_time)))\n",
    "                target_time = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "                pred, _, _ = model(x1, x2, y)\n",
    "                score = change_model(pred)\n",
    "                \n",
    "                #特征变换\n",
    "                alignment_mat = score[0]\n",
    "                l_mat = score[1]\n",
    "                r_mat = score[2]\n",
    "\n",
    "                I = torch.eye(batch_size2)\n",
    "                allone = torch.ones(batch_size2, batch_size2)\n",
    "                mask = allone - 2 * I\n",
    "\n",
    "                l_reg = torch.mm(l_mat * I, torch.ones(batch_size2, 1))\n",
    "                r_reg = torch.mm(r_mat * I, torch.ones(batch_size2, 1))\n",
    "                offset_pred = torch.cat([l_reg, r_reg], 1)\n",
    "\n",
    "                loss_mat = torch.log(allone + torch.exp(mask * score[0] + epsilon))\n",
    "\n",
    "                para = I + 1.0 / batch_size2 * allone\n",
    "                loss_mat = loss_mat * para\n",
    "                loss_alignment = loss_mat.mean()\n",
    "                loss_reg = torch.abs(offset_pred - target_time).mean()\n",
    "                \n",
    "                #最终的损失函数\n",
    "                Loss = loss_reg * lamba + loss_alignment\n",
    "                \n",
    "                train_regloss_sum += loss_reg\n",
    "                train_loss_sum += Loss\n",
    "                train_alignmentloss_sum += loss_alignment\n",
    "                optimizer.zero_grad()\n",
    "                Loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())        \n",
    "        line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.6f | loss_align: %.6f | loss_reg: %.6f' % (epoch, train_loss_sum / (count), train_alignmentloss_sum / (count), train_regloss_sum / (count)))\n",
    "        print(line)\n",
    "        write_resualt_file(resualt_path, line)\n",
    "        train_loss_2_list.append(train_loss_sum / count)\n",
    "\n",
    "        if (epoch+1)%5==0:\n",
    "            print(\"进行验证.......\")\n",
    "            begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            batch_val = batch_iter(x_batch_val, x_csv_val, y_csv_val,source_list_val,target_list_val, batch_size2)\n",
    "            count = 0\n",
    "            val_loss_sum = 0\n",
    "            val_alignmentloss_sum =0\n",
    "            val_regloss_sum =0\n",
    "            for x_batch, x_csv, y_csv, source_time, target_time in batch_val:\n",
    "                if x_csv.shape[0]==batch_size2:\n",
    "                    count += 1\n",
    "                    x1 = Variable(torch.LongTensor(x_batch))\n",
    "                    x2 = Variable(torch.FloatTensor(np.array(x_csv)))\n",
    "                    y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                    source_time = Variable(torch.FloatTensor(np.array(source_time)))\n",
    "                    target_time = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "                    pred, _, _ = model(x1, x2, y)\n",
    "                    score = change_model(pred)\n",
    "                    \n",
    "                    #特征变换\n",
    "                    alignment_mat = score[0]\n",
    "                    l_mat = score[1]\n",
    "                    r_mat = score[2]\n",
    "\n",
    "                    I = torch.eye(batch_size2)\n",
    "                    allone = torch.ones(batch_size2, batch_size2)\n",
    "                    mask = allone - 2 * I\n",
    "\n",
    "                    l_reg = torch.mm(l_mat * I, torch.ones(batch_size2, 1))\n",
    "                    r_reg = torch.mm(r_mat * I, torch.ones(batch_size2, 1))\n",
    "                    offset_pred = torch.cat([l_reg, r_reg], 1)\n",
    "                    loss_mat = torch.log(allone + torch.exp(mask * score[0] + epsilon))\n",
    "\n",
    "                    para = I + 1.0 / batch_size2 * allone\n",
    "                    loss_mat = loss_mat * para\n",
    "                    loss_alignment = loss_mat.mean()\n",
    "                    loss_reg = torch.abs(offset_pred - target_time).mean()\n",
    "                    \n",
    "                    #模型最终的loss\n",
    "                    Loss = loss_reg * lamba + loss_alignment\n",
    "                    \n",
    "                    val_alignmentloss_sum += loss_alignment\n",
    "                    val_regloss_sum += loss_reg\n",
    "                    val_loss_sum += Loss\n",
    "                    optimizer.zero_grad()\n",
    "                    Loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "            line = begin_time+' | '+current_time+(' | Epoch: %3d | Loss: %.6f | loss_align: %.6f | loss_reg: %.6f' % (epoch, val_loss_sum / (count), val_alignmentloss_sum / (count), val_regloss_sum / (count)))\n",
    "            print(line)\n",
    "            write_resualt_file(resualt_path, line)\n",
    "            val_loss_2_list.append(val_loss_sum / count) \n",
    "            torch.save(change_model.state_dict(), model_save_path + 'epoch_'+str(inital_epoch + epoch)+'.pkl')\n",
    "            if (val_loss_sum / count ) < best_val_loss_2:\n",
    "                best_val_loss_2 = val_loss_sum / count\n",
    "                print(\"model save!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_for_Change_model_3(save_resualt_path2, save_path2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begin......\n",
      "2020-08-19 01:47:35 | 2020-08-19 01:48:50 | Epoch:   0 | Loss: 1.188553 | loss_align: 0.000000 | loss_reg: 118.855247\n",
      "2020-08-19 01:48:50 | 2020-08-19 01:50:01 | Epoch:   1 | Loss: 1.138591 | loss_align: 0.000000 | loss_reg: 113.859100\n",
      "2020-08-19 01:50:01 | 2020-08-19 01:51:10 | Epoch:   2 | Loss: 1.169097 | loss_align: 0.000000 | loss_reg: 116.909531\n",
      "2020-08-19 01:51:10 | 2020-08-19 01:52:22 | Epoch:   3 | Loss: 1.136061 | loss_align: 0.000000 | loss_reg: 113.606087\n",
      "2020-08-19 01:52:22 | 2020-08-19 01:53:30 | Epoch:   4 | Loss: 1.164194 | loss_align: 0.000000 | loss_reg: 116.419464\n",
      "进行验证.......\n",
      "2020-08-19 01:53:30 | 2020-08-19 02:06:38 | Epoch:   4 | Loss: 1.066642 | loss_align: 0.000000 | loss_reg: 106.664253\n",
      "model save!\n",
      "2020-08-19 02:06:38 | 2020-08-19 02:07:45 | Epoch:   5 | Loss: 1.208310 | loss_align: 0.000000 | loss_reg: 120.830971\n",
      "2020-08-19 02:07:45 | 2020-08-19 02:08:49 | Epoch:   6 | Loss: 1.162870 | loss_align: 0.000000 | loss_reg: 116.286926\n",
      "2020-08-19 02:08:49 | 2020-08-19 02:09:53 | Epoch:   7 | Loss: 1.226824 | loss_align: 0.000000 | loss_reg: 122.682503\n",
      "2020-08-19 02:09:53 | 2020-08-19 02:10:56 | Epoch:   8 | Loss: 1.154341 | loss_align: 0.000000 | loss_reg: 115.434189\n",
      "2020-08-19 02:10:56 | 2020-08-19 02:11:59 | Epoch:   9 | Loss: 1.201612 | loss_align: 0.000000 | loss_reg: 120.161293\n",
      "进行验证.......\n",
      "2020-08-19 02:11:59 | 2020-08-19 02:12:17 | Epoch:   9 | Loss: 1.061803 | loss_align: 0.000000 | loss_reg: 106.180183\n",
      "model save!\n",
      "2020-08-19 02:12:17 | 2020-08-19 02:13:23 | Epoch:  10 | Loss: 1.141708 | loss_align: 0.000000 | loss_reg: 114.170868\n",
      "2020-08-19 02:13:23 | 2020-08-19 02:14:31 | Epoch:  11 | Loss: 1.143543 | loss_align: 0.000000 | loss_reg: 114.354370\n",
      "2020-08-19 02:14:31 | 2020-08-19 02:15:40 | Epoch:  12 | Loss: 1.183944 | loss_align: 0.000000 | loss_reg: 118.394630\n",
      "2020-08-19 02:15:40 | 2020-08-19 02:16:52 | Epoch:  13 | Loss: 1.162871 | loss_align: 0.000000 | loss_reg: 116.287315\n",
      "2020-08-19 02:16:52 | 2020-08-19 02:18:06 | Epoch:  14 | Loss: 1.151208 | loss_align: 0.000000 | loss_reg: 115.120750\n",
      "进行验证.......\n",
      "2020-08-19 02:18:06 | 2020-08-19 02:18:26 | Epoch:  14 | Loss: 1.039113 | loss_align: 0.000000 | loss_reg: 103.911331\n",
      "model save!\n",
      "2020-08-19 02:18:26 | 2020-08-19 02:19:41 | Epoch:  15 | Loss: 1.158634 | loss_align: 0.000000 | loss_reg: 115.863480\n",
      "2020-08-19 02:19:41 | 2020-08-19 02:21:01 | Epoch:  16 | Loss: 1.194152 | loss_align: 0.000000 | loss_reg: 119.415337\n",
      "2020-08-19 02:21:01 | 2020-08-19 02:22:24 | Epoch:  17 | Loss: 1.180010 | loss_align: 0.000000 | loss_reg: 118.000946\n",
      "2020-08-19 02:22:24 | 2020-08-19 02:23:51 | Epoch:  18 | Loss: 1.190126 | loss_align: 0.000000 | loss_reg: 119.012489\n",
      "2020-08-19 02:23:51 | 2020-08-19 02:25:06 | Epoch:  19 | Loss: 1.142261 | loss_align: 0.000000 | loss_reg: 114.226120\n",
      "进行验证.......\n",
      "2020-08-19 02:25:06 | 2020-08-19 02:25:24 | Epoch:  19 | Loss: 1.067577 | loss_align: 0.000000 | loss_reg: 106.757729\n",
      "2020-08-19 02:25:24 | 2020-08-19 02:26:29 | Epoch:  20 | Loss: 1.178208 | loss_align: 0.000000 | loss_reg: 117.820839\n",
      "2020-08-19 02:26:29 | 2020-08-19 02:27:35 | Epoch:  21 | Loss: 1.189966 | loss_align: 0.000000 | loss_reg: 118.996582\n",
      "2020-08-19 02:27:35 | 2020-08-19 02:28:40 | Epoch:  22 | Loss: 1.157138 | loss_align: 0.000000 | loss_reg: 115.713715\n",
      "2020-08-19 02:28:40 | 2020-08-19 02:29:47 | Epoch:  23 | Loss: 1.169244 | loss_align: 0.000000 | loss_reg: 116.924400\n",
      "2020-08-19 02:29:47 | 2020-08-19 02:30:52 | Epoch:  24 | Loss: 1.168551 | loss_align: 0.000000 | loss_reg: 116.855042\n",
      "进行验证.......\n",
      "2020-08-19 02:30:52 | 2020-08-19 02:31:10 | Epoch:  24 | Loss: 1.066561 | loss_align: 0.000000 | loss_reg: 106.656097\n",
      "2020-08-19 02:31:10 | 2020-08-19 02:32:15 | Epoch:  25 | Loss: 1.158746 | loss_align: 0.000000 | loss_reg: 115.874596\n",
      "2020-08-19 02:32:15 | 2020-08-19 02:33:21 | Epoch:  26 | Loss: 1.172508 | loss_align: 0.000000 | loss_reg: 117.250656\n",
      "2020-08-19 02:33:21 | 2020-08-19 02:34:27 | Epoch:  27 | Loss: 1.173131 | loss_align: 0.000000 | loss_reg: 117.313072\n",
      "2020-08-19 02:34:27 | 2020-08-19 02:35:33 | Epoch:  28 | Loss: 1.162991 | loss_align: 0.000000 | loss_reg: 116.299065\n",
      "2020-08-19 02:35:33 | 2020-08-19 02:36:39 | Epoch:  29 | Loss: 1.149310 | loss_align: 0.000000 | loss_reg: 114.930908\n",
      "进行验证.......\n",
      "2020-08-19 02:36:39 | 2020-08-19 02:36:57 | Epoch:  29 | Loss: 1.038701 | loss_align: 0.000000 | loss_reg: 103.870140\n",
      "model save!\n",
      "2020-08-19 02:36:57 | 2020-08-19 02:38:02 | Epoch:  30 | Loss: 1.172164 | loss_align: 0.000000 | loss_reg: 117.216461\n",
      "2020-08-19 02:38:02 | 2020-08-19 02:39:07 | Epoch:  31 | Loss: 1.147845 | loss_align: 0.000000 | loss_reg: 114.784500\n",
      "2020-08-19 02:39:07 | 2020-08-19 02:40:14 | Epoch:  32 | Loss: 1.146950 | loss_align: 0.000000 | loss_reg: 114.695053\n",
      "2020-08-19 02:40:14 | 2020-08-19 02:41:20 | Epoch:  33 | Loss: 1.124360 | loss_align: 0.000000 | loss_reg: 112.435913\n",
      "2020-08-19 02:41:20 | 2020-08-19 02:42:26 | Epoch:  34 | Loss: 1.217931 | loss_align: 0.000000 | loss_reg: 121.793129\n",
      "进行验证.......\n",
      "2020-08-19 02:42:26 | 2020-08-19 02:42:45 | Epoch:  34 | Loss: 1.062739 | loss_align: 0.000000 | loss_reg: 106.273911\n",
      "2020-08-19 02:42:45 | 2020-08-19 02:43:51 | Epoch:  35 | Loss: 1.161049 | loss_align: 0.000000 | loss_reg: 116.104897\n",
      "2020-08-19 02:43:51 | 2020-08-19 02:44:57 | Epoch:  36 | Loss: 1.145499 | loss_align: 0.000000 | loss_reg: 114.549805\n",
      "2020-08-19 02:44:57 | 2020-08-19 02:46:03 | Epoch:  37 | Loss: 1.141819 | loss_align: 0.000000 | loss_reg: 114.181992\n",
      "2020-08-19 02:46:03 | 2020-08-19 02:47:09 | Epoch:  38 | Loss: 1.156527 | loss_align: 0.000000 | loss_reg: 115.652618\n",
      "2020-08-19 02:47:09 | 2020-08-19 02:48:16 | Epoch:  39 | Loss: 1.172563 | loss_align: 0.000000 | loss_reg: 117.256470\n",
      "进行验证.......\n",
      "2020-08-19 02:48:16 | 2020-08-19 02:48:36 | Epoch:  39 | Loss: 1.056437 | loss_align: 0.000000 | loss_reg: 105.643684\n",
      "2020-08-19 02:48:36 | 2020-08-19 02:49:43 | Epoch:  40 | Loss: 1.180437 | loss_align: 0.000000 | loss_reg: 118.043633\n",
      "2020-08-19 02:49:43 | 2020-08-19 02:50:49 | Epoch:  41 | Loss: 1.148997 | loss_align: 0.000000 | loss_reg: 114.899811\n",
      "2020-08-19 02:50:49 | 2020-08-19 02:51:56 | Epoch:  42 | Loss: 1.167591 | loss_align: 0.000000 | loss_reg: 116.759254\n",
      "2020-08-19 02:51:56 | 2020-08-19 02:53:03 | Epoch:  43 | Loss: 1.172219 | loss_align: 0.000000 | loss_reg: 117.221848\n",
      "2020-08-19 02:53:03 | 2020-08-19 02:54:09 | Epoch:  44 | Loss: 1.151847 | loss_align: 0.000000 | loss_reg: 115.184685\n",
      "进行验证.......\n",
      "2020-08-19 02:54:09 | 2020-08-19 02:54:28 | Epoch:  44 | Loss: 1.135861 | loss_align: 0.000000 | loss_reg: 113.586121\n",
      "2020-08-19 02:54:28 | 2020-08-19 02:55:35 | Epoch:  45 | Loss: 1.155322 | loss_align: 0.000000 | loss_reg: 115.532288\n",
      "2020-08-19 02:55:35 | 2020-08-19 02:56:41 | Epoch:  46 | Loss: 1.185817 | loss_align: 0.000000 | loss_reg: 118.581635\n",
      "2020-08-19 02:56:41 | 2020-08-19 02:57:48 | Epoch:  47 | Loss: 1.175468 | loss_align: 0.000000 | loss_reg: 117.546989\n",
      "2020-08-19 02:57:48 | 2020-08-19 02:58:55 | Epoch:  48 | Loss: 1.131581 | loss_align: 0.000000 | loss_reg: 113.158134\n",
      "2020-08-19 02:58:55 | 2020-08-19 03:00:01 | Epoch:  49 | Loss: 1.192602 | loss_align: 0.000000 | loss_reg: 119.260155\n",
      "进行验证.......\n",
      "2020-08-19 03:00:01 | 2020-08-19 03:00:20 | Epoch:  49 | Loss: 1.122313 | loss_align: 0.000000 | loss_reg: 112.231255\n",
      "2020-08-19 03:00:20 | 2020-08-19 03:01:27 | Epoch:  50 | Loss: 1.152262 | loss_align: 0.000000 | loss_reg: 115.226257\n",
      "2020-08-19 03:01:27 | 2020-08-19 03:02:34 | Epoch:  51 | Loss: 1.174082 | loss_align: 0.000000 | loss_reg: 117.408218\n",
      "2020-08-19 03:02:34 | 2020-08-19 03:03:41 | Epoch:  52 | Loss: 1.135007 | loss_align: 0.000000 | loss_reg: 113.500740\n",
      "2020-08-19 03:03:41 | 2020-08-19 03:04:48 | Epoch:  53 | Loss: 1.174918 | loss_align: 0.000000 | loss_reg: 117.491844\n",
      "2020-08-19 03:04:48 | 2020-08-19 03:05:55 | Epoch:  54 | Loss: 1.274553 | loss_align: 0.000000 | loss_reg: 127.455353\n",
      "进行验证.......\n",
      "2020-08-19 03:05:55 | 2020-08-19 03:06:14 | Epoch:  54 | Loss: 1.149595 | loss_align: 0.000000 | loss_reg: 114.959343\n",
      "2020-08-19 03:06:14 | 2020-08-19 03:07:21 | Epoch:  55 | Loss: 1.169621 | loss_align: 0.000000 | loss_reg: 116.962212\n",
      "2020-08-19 03:07:21 | 2020-08-19 03:08:28 | Epoch:  56 | Loss: 1.139158 | loss_align: 0.000000 | loss_reg: 113.915878\n",
      "2020-08-19 03:08:28 | 2020-08-19 03:09:35 | Epoch:  57 | Loss: 1.171533 | loss_align: 0.000000 | loss_reg: 117.153313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-5ac741c3023d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path_for_two_CNN_resualt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path_for_two_CNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-0ef81da09609>\u001b[0m in \u001b[0;36mtrain_2\u001b[1;34m(resualt_path, model_save_path, inital_epoch)\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0msource_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mtarget_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchange_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-b485d8d8f56e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2, x3)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mlstm_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    169\u001b[0m             hx = input.new_zeros(self.num_layers * num_directions,\n\u001b[0;32m    170\u001b[0m                                  \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                                  requires_grad=False)\n\u001b[0m\u001b[0;32m    172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LSTM'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_2(save_path_for_two_CNN_resualt, save_path_for_two_CNN, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAERCAYAAAB7FtAjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIQiCIAiASAiCIAjCQ/8PpDsYYLxu/mcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(train_loss_2_list, val_loss_2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算R@1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IoU_thresh = [0.1, 0.3, 0.5, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_IoU(i0, i1):\n",
    "    # calculate temporal intersection over union\n",
    "    union = (min(i0[0], i1[0]), max(i0[1], i1[1]))\n",
    "    inter = (max(i0[0], i1[0]), min(i0[1], i1[1]))\n",
    "    iou = 1.0*(inter[1]-inter[0])/(union[1]-union[0])\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_IoU_recall_top_n_forreg(iou_thresh, time_mat, time_pre_mat):#top\n",
    "    correct_num = 0\n",
    "    for i in range(time_mat.shape[0]):\n",
    "        gt_start = time_mat[i][0]\n",
    "        gt_end = time_mat[i][1]\n",
    "        pred_start = time_pre_mat[i][0]\n",
    "        pred_end = time_pre_mat[i][1]\n",
    "        iou = calculate_IoU((gt_start, gt_end),(pred_start, pred_end))\n",
    "        if iou>=iou_thresh:\n",
    "            correct_num+=1\n",
    "    return correct_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取test集\n",
    "# 读取test数据集\n",
    "def get_test_dict_for_My_model(path, csv_path):\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    data_id = []\n",
    "    csv=[]\n",
    "    time_list=[]\n",
    "    Max_len=-1\n",
    "    count=0\n",
    "    with open(path) as contents:\n",
    "        for line in contents:\n",
    "            count+=1\n",
    "            List = line.split('#')\n",
    "            video_name = List[0]\n",
    "            time_length = float(List[1])\n",
    "            foldtype = List[2]\n",
    "            recipetype = List[3]\n",
    "            target = List[4]\n",
    "            \n",
    "            #将句子转换为id表示：\n",
    "            sentence = List[6].strip('\\n').strip()\n",
    "            sentence = re.split(r\"[,| |.]\",sentence)\n",
    "            sentence_id = [word_to_id[x] for x in sentence if x in word_to_id]\n",
    "            if len(sentence_id) > Max_len:\n",
    "                Max_len = len(sentence_id)\n",
    "            data_id.append(sentence_id)\n",
    "            \n",
    "            #寻找路径,先统一取0001\n",
    "            dir_path = csv_path+'/'+foldtype+'/'+recipetype+'/'+video_name+'/0001/'\n",
    "            name = os.listdir(dir_path)[0]\n",
    "            dir_path = dir_path + name\n",
    "            \n",
    "            #读取csv文件\n",
    "            my_file = Path(dir_path)\n",
    "            if my_file.exists():\n",
    "                frame_sum = pd.read_csv(dir_path, header=None)\n",
    "            else:\n",
    "                print(\"目录不存在！\")\n",
    "            \n",
    "            #确定时间点，前帧后帧取pooling\n",
    "            target = target.split('_')\n",
    "            cur_start = float(target[0])\n",
    "            cur_end = float(target[1])\n",
    "            middle_time = (cur_start + cur_end)//2\n",
    "            \n",
    "            #中间帧\n",
    "            target_frame_num = int(middle_time/time_length*500)\n",
    "            target_middle_frame = frame_sum.loc[target_frame_num]\n",
    "            \n",
    "            csv.append([target_middle_frame])\n",
    "            time_list.append([cur_start, cur_end])\n",
    "            \n",
    "    #将所有的句子pad为同一最大长度\n",
    "    batch_data_id = np.array([line +[0]*(Max_len-len(line)) \n",
    "                                            for line in data_id])\n",
    "    batch_seq = torch.LongTensor(batch_data_id)    \n",
    "    print(len(batch_seq),len(csv),len(time_list))\n",
    "    \n",
    "    return batch_seq, csv, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523 523 523\n"
     ]
    }
   ],
   "source": [
    "My_test_seq, My_test_csv, My_test_time_list = get_test_dict_for_My_model(val_path, csv_path)\n",
    "#x_batch_train, x_csv_train, y_csv_train, source_list_train, target_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTM_CNN:\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([768, 64]).\n\tsize mismatch for rnn.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([768, 64]).\n\tsize mismatch for rnn.weight_hh_l0_reverse: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l0_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l0_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.weight_ih_l1: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([768, 512]).\n\tsize mismatch for rnn.weight_hh_l1: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.weight_ih_l1_reverse: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([768, 512]).\n\tsize mismatch for rnn.weight_hh_l1_reverse: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l1_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l1_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ed584610ddfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmiddle_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmiddle_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/wuxun/Desktop/Data/save_model/new_model_for_1500/394_params.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mMy_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#My_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2无alignment/513.pkl'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mMy_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2有alignment/epoch_178.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 769\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTM_CNN:\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([768, 64]).\n\tsize mismatch for rnn.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([768, 64]).\n\tsize mismatch for rnn.weight_hh_l0_reverse: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l0_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l0_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.weight_ih_l1: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([768, 512]).\n\tsize mismatch for rnn.weight_hh_l1: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.weight_ih_l1_reverse: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([768, 512]).\n\tsize mismatch for rnn.weight_hh_l1_reverse: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for rnn.bias_ih_l1_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for rnn.bias_hh_l1_reverse: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768])."
     ]
    }
   ],
   "source": [
    "middle_model = LSTM_CNN()\n",
    "middle_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/new_model_for_1500/394_params.pkl'))\n",
    "My_model = Change()\n",
    "#My_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2无alignment/513.pkl'))\n",
    "My_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model2/新训练的model1以后训练model2有alignment/epoch_178.pkl'))\n",
    "My_model.eval()\n",
    "middle_model.eval()                                                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-87e987a43dfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMy_test_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mMy_time_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMy_test_time_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mMy_output1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmiddle_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMy_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMy_output1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-89f150f776fc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#lstm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mlstm_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    129\u001b[0m             raise RuntimeError(\n\u001b[0;32m    130\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m--> 131\u001b[1;33m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "batch_size2 = 1\n",
    "for i in range(My_test_seq.shape[0]):\n",
    "    \n",
    "    x = Variable(torch.LongTensor(My_test_seq[i]))\n",
    "    y = Variable(torch.FloatTensor(np.array(My_test_csv[i])))\n",
    "    My_time_mat = Variable(torch.FloatTensor(np.array(My_test_time_list[i])))\n",
    "    My_output1 = middle_model(x, y)\n",
    "    score = My_model(My_output1)\n",
    "\n",
    "    #特征变换\n",
    "    alignment_mat = score[0]\n",
    "    l_mat = score[1]\n",
    "    r_mat = score[2]\n",
    "\n",
    "    I = torch.eye(batch_size2)\n",
    "    allone = torch.ones(batch_size2, batch_size2)\n",
    "    mask = allone - 2 * I\n",
    "\n",
    "    l_reg = torch.mm(l_mat * I, torch.ones(batch_size2, 1))\n",
    "    r_reg = torch.mm(r_mat * I, torch.ones(batch_size2, 1))\n",
    "    offset_pred = torch.cat([l_reg, r_reg], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_model: R@1 for iou_thresh: 0.1 is : 0.065\n",
      "My_model: R@1 for iou_thresh: 0.3 is : 0.025\n",
      "My_model: R@1 for iou_thresh: 0.5 is : 0.010\n",
      "My_model: R@1 for iou_thresh: 0.7 is : 0.002\n"
     ]
    }
   ],
   "source": [
    "My_pred_time_mat = offset_pred\n",
    "for iou_thresh in IoU_thresh:\n",
    "    corrnum = compute_IoU_recall_top_n_forreg(iou_thresh, My_time_mat, My_pred_time_mat)\n",
    "    corr_avg = corrnum*1.0 / My_time_mat.shape[0] \n",
    "    print(\"My_model: R@1 for iou_thresh: %.1f is : %.3f\" % (iou_thresh, corr_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算偏移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list_time = []\n",
    "for i in range(My_pred_time_mat.shape[0]):\n",
    "    begin = My_pred_time_mat[i][0].detach().numpy()\n",
    "    end = My_pred_time_mat[i][1].detach().numpy()\n",
    "    pred_list_time.append((begin + end)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_time = []\n",
    "for i in range(My_time_mat.shape[0]):\n",
    "    begin = My_time_mat[i][0].detach().numpy()\n",
    "    end = My_time_mat[i][1].detach().numpy()\n",
    "    list_time.append((begin + end)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#计算偏移值\n",
    "avg = 0.0\n",
    "offset_list = []\n",
    "for i in range(len(list_time)):\n",
    "    offset = list_time[i] - pred_list_time[i]\n",
    "    offset_list.append(offset)\n",
    "    avg = avg + offset\n",
    "avg = avg /len(list_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.96427903439757\n"
     ]
    }
   ],
   "source": [
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IoU_thresh = [0.7, 0.5, 0.3, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/resualt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "d1=np.array([0.159, 0.061, 0.008, 0.000])#扩充了  batch不为1 无align\n",
    "d2=np.array([0.187, 0.098, 0.031, 0.008]) #扩充了  batch为1  有align\n",
    "d3=np.array([0.159, 0.101, 0.044, 0.019])#无扩充  batch为1\n",
    "d4=np.array([0.152, 0.098, 0.038, 0.011]) #无扩充 batch不为1  无align\n",
    "\n",
    "t1=np.array([0.106, 0.061, 0.030, 0.015])# bacth不为1 扩充了\n",
    "t2=np.array([0.114, 0.076, 0.015, 0.000])#batch=1 无扩充\n",
    "print(type(d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resualt_dict = {'dataset_augment_with_align':d1[i],'dataset_augment_without_align':d2[i],'dataset_no_augment_with align':d3[i],'dataset_no_augment_without_align':d4[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(1,5): \n",
    "#     writer.add_scalars('resualt_for_model', {'dataset_augment_with_align':d1[i],'dataset_augment_without_align':d2[i],'dataset_no_augment_with align':d3[i],'dataset_no_augment_without_align':d4[i]} , global_step =i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0.7, 0.1,4)\n",
    "print(type(x))\n",
    "state_dict = dict({0.7:1, 0.5:2, 0.3:3, 0.1:4})\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(i):\n",
    "    if i ==0:\n",
    "        return d1\n",
    "    elif i==1:\n",
    "        return d2\n",
    "    elif i==2:\n",
    "        return d3\n",
    "    elif i==3:\n",
    "        return d4\n",
    "    \n",
    "def TALL_model(i):\n",
    "    if i==0:\n",
    "        return t1\n",
    "    elif i==1:\n",
    "        return t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wuxun\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "name_list = ['扩充且有align损失','扩充且无align损失','无扩充且有align损失','无扩充且无align损失']\n",
    "x = np.linspace(0.1, 0.7, 4)\n",
    "with plt.style.context(['science','ieee','no-latex']):\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(name_list)):\n",
    "        ax.plot(x, model(i),label=name_list[i])\n",
    "    ax.legend(title='kinds_of_our_model')\n",
    "    ax.set(xlabel='IoU_thresh')\n",
    "    ax.set(ylabel='R@1')\n",
    "    ax.autoscale(tight=True)\n",
    "    fig.savefig('My_model.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wuxun\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "TALL_name_list = ['TALL扩充且batch不为1','TALL无扩充batch为1']\n",
    "x = np.linspace(0.1, 0.7, 4)\n",
    "with plt.style.context(['science','ieee','no-latex']):\n",
    "    fig, ax = plt.subplots()\n",
    "    #plt.legend(loc='lower right', fontsize=100) # 标签位置\n",
    "    for i in range(len(TALL_name_list)):\n",
    "        ax.plot(x, TALL_model(i),label=TALL_name_list[i])\n",
    "    ax.legend(title='kinds_of_tall_model')\n",
    "    ax.set(xlabel='IoU_thresh')\n",
    "    ax.set(ylabel='R@1')\n",
    "    ax.autoscale(tight=True)\n",
    "    fig.savefig('TALL_model.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
