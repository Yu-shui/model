{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TALL模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime  # 用于计算时间\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "#import tensorflow.contrib.keras as kr\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from torchtext import data\n",
    "import jieba\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import tensorwatch as tw\n",
    "import torchvision.models\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(precision=15)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = 'C:/Users/wuxun/Desktop/Data/TALL/traning/training.txt'\n",
    "val_path = 'C:/Users/wuxun/Desktop/Data/TALL/validation/validation.txt'\n",
    "vocab_dir = 'C:/Users/wuxun/Desktop/Data/vocab.txt'\n",
    "csv_path = 'D:/csv/'\n",
    "save_path = 'C:/Users/wuxun/Desktop/Data/TALL/save_model/'\n",
    "test_path = 'C:/Users/wuxun/Desktop/Data/TALL/testing/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vocab(vocab_dir):\n",
    "\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "\n",
    "    with open(vocab_dir) as fp:\n",
    "        words = [(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dict(path, csv_path):\n",
    "    '''\n",
    "    获得最终的数据集\n",
    "    path:文本数据集\n",
    "    csv_path\n",
    "    '''\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    data_id = []\n",
    "    target_csv=[]\n",
    "    target_time_list=[]\n",
    "    Max_len=-1\n",
    "    count=0\n",
    "    with open(path) as contents:\n",
    "        for line in contents:\n",
    "            count+=1\n",
    "            List = line.split('#')\n",
    "            video_name = List[0]\n",
    "            time_length = float(List[1])\n",
    "            foldtype = List[2]\n",
    "            recipetype = List[3]\n",
    "            target = List[4]\n",
    "            \n",
    "            #将句子转换为id表示：\n",
    "            sentence = List[6].strip('\\n').strip()\n",
    "            sentence = re.split(r\"[,| |.]\",sentence)\n",
    "            sentence_id = [word_to_id[x] for x in sentence if x in word_to_id]\n",
    "            if len(sentence_id) > Max_len:\n",
    "                Max_len = len(sentence_id)\n",
    "            data_id.append(sentence_id)\n",
    "            \n",
    "            #寻找路径,先统一取0001\n",
    "            dir_path = csv_path+'/'+foldtype+'/'+recipetype+'/'+video_name+'/0001/'\n",
    "            name = os.listdir(dir_path)[0]\n",
    "            dir_path = dir_path + name\n",
    "            \n",
    "            #读取csv文件\n",
    "            my_file = Path(dir_path)\n",
    "            if my_file.exists():\n",
    "                frame_sum = pd.read_csv(dir_path, header=None)\n",
    "            else:\n",
    "                print(\"目录不存在！\")\n",
    "            \n",
    "            #确定时间点，前帧后帧取pooling\n",
    "            target = target.split('_')\n",
    "            cur_start = float(target[0])\n",
    "            cur_end = float(target[1])\n",
    "            middle_time = (cur_start + cur_end)//2\n",
    "            \n",
    "            #中间帧\n",
    "            target_frame_num = int(middle_time/time_length*500)\n",
    "            target_middle_frame = frame_sum.loc[target_frame_num]\n",
    "            \n",
    "            #上下文信息\n",
    "            target_frame_start = int(cur_start/time_length*500)\n",
    "            target_frame_end = int(cur_end/time_length*500)\n",
    "            if target_frame_start == target_frame_num:\n",
    "                print(str(cur_start)+\"  \"+str(cur_end)+\" \"+str(time_length))\n",
    "                print(\"出现重复！\")\n",
    "                target_frame_start = min(target_frame_num - 3, 0)\n",
    "            if  target_frame_end ==target_frame_num:\n",
    "                print(str(cur_start)+\"  \"+str(cur_end)+\" \"+str(time_length))\n",
    "                print(\"出现重复！\")\n",
    "                target_frame_start = min(target_frame_num + 3, 499)\n",
    "                \n",
    "            pre_context = np.zeros([target_frame_num - target_frame_start, 512], dtype=np.float32)\n",
    "            post_context = np.zeros([target_frame_end - target_frame_num, 512], dtype=np.float32) \n",
    "            for i in range(target_frame_num - target_frame_start):\n",
    "                pre_context[i] = frame_sum.loc[i]\n",
    "                \n",
    "            for i in range(target_frame_end - target_frame_num):\n",
    "                post_context[i] = frame_sum.loc[i]\n",
    "            \n",
    "            #对pre_context和post_context取均值\n",
    "            pre_context = np.mean(pre_context, axis=0)\n",
    "            post_context = np.mean(post_context, axis=0)\n",
    "            \n",
    "            #对三段信息进行拼接\n",
    "            image = np.hstack((pre_context, target_middle_frame, post_context))\n",
    "            \n",
    "            target_csv.append(image)\n",
    "            target_time_list.append([cur_start, cur_end])\n",
    "            \n",
    "    #将所有的句子pad为同一最大长度\n",
    "    batch_data_id = np.array([line +[0]*(Max_len-len(line)) \n",
    "                                            for line in data_id])\n",
    "    batch_seq = torch.LongTensor(batch_data_id)    \n",
    "    print(len(batch_seq),len(target_csv),len(target_time_list))\n",
    "    \n",
    "    return batch_seq, target_csv, target_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.0  107.0 219.22\n",
      "出现重复！\n",
      "50.0  51.0 244.37\n",
      "出现重复！\n",
      "87.0  88.0 244.37\n",
      "出现重复！\n",
      "30.0  31.0 129.24\n",
      "出现重复！\n",
      "258.0  259.0 396.3\n",
      "出现重复！\n",
      "31.0  32.0 88.22\n",
      "出现重复！\n",
      "51.0  52.0 271.07\n",
      "出现重复！\n",
      "1548 1548 1548\n"
     ]
    }
   ],
   "source": [
    "sentence_tarin_batch, target_train_csv, target_train_list = get_dict(train_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146.0  148.0 561.49\n",
      "出现重复！\n",
      "262.0  263.0 352.4\n",
      "出现重复！\n",
      "183.0  184.0 329.89\n",
      "出现重复！\n",
      "281.0  282.0 349.68\n",
      "出现重复！\n",
      "523 523 523\n"
     ]
    }
   ],
   "source": [
    "sentence_val_batch, target_val_csv, target_val_list = get_dict(val_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(x_batch, target_csv, target_list, batch_size = 64):\n",
    "    \"\"\"\n",
    "    生成批次数据\n",
    "    \"\"\"\n",
    "    data_len = x_batch.shape[0]\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    \n",
    "    x_batch_shuffle = x_batch[indices]\n",
    "    y_csv_shuffle = np.array(target_csv)[indices]\n",
    "    target_list = np.array(target_list)[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_batch_shuffle[start_id:end_id], y_csv_shuffle[start_id:end_id], target_list[start_id:end_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, mean=0, std=0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TALL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TALL, self).__init__()\n",
    "        self.semantic_size = 128 # the size of visual and semantic comparison size\n",
    "        #self.sentence_embedding_size = 4800\n",
    "        self.visual_feature_dim = 1536\n",
    "        self.v2s_lt = nn.Linear(self.visual_feature_dim, self.semantic_size)#视觉提取\n",
    "        self.s2s_lt = nn.Linear(256, 128)#句子提取\n",
    "        self.fc1 = torch.nn.Conv2d(128*4, 128, kernel_size=1, stride=1)\n",
    "        self.fc2 = torch.nn.Conv2d(128, 3, kernel_size=1, stride=1)\n",
    "        # Initializing weights\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "        #self_add\n",
    "        self.embedding = nn.Embedding(5000, 64)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "\n",
    "    def cross_modal_comb(self, visual_feat, sentence_embed):\n",
    "        batch_size = visual_feat.size(0)\n",
    "        #print(\"进入交叉阶段,batch_size = \"+str(batch_size))\n",
    "\n",
    "        vv_feature = visual_feat.expand([batch_size,batch_size,self.semantic_size])\n",
    "        ss_feature = sentence_embed.repeat(1,1,batch_size).view(batch_size,batch_size,self.semantic_size)\n",
    "\n",
    "        concat_feature = torch.cat([vv_feature, ss_feature], 2)\n",
    "\n",
    "        mul_feature = vv_feature * ss_feature # 64,64,128\n",
    "        add_feature = vv_feature + ss_feature # 64,64,128\n",
    "\n",
    "        comb_feature = torch.cat([mul_feature, add_feature, concat_feature], 2)\n",
    "\n",
    "        return comb_feature\n",
    "\n",
    "\n",
    "    def forward(self, visual_feature_train, sentence_embed_train):\n",
    "        transformed_clip_train = self.v2s_lt(visual_feature_train)\n",
    "        transformed_clip_train_norm = F.normalize(transformed_clip_train, p=2, dim=1)\n",
    "        \n",
    "        sentence_embed_train = self.embedding(sentence_embed_train)\n",
    "        sentence_embed_train,_ =self.lstm(sentence_embed_train)\n",
    "        transformed_sentence_train = self.s2s_lt(sentence_embed_train[:,-1,:])\n",
    "        transformed_sentence_train_norm = F.normalize(transformed_sentence_train, p=2, dim=1)\n",
    "        \n",
    "        cross_modal_vec_train = self.cross_modal_comb(transformed_clip_train_norm, transformed_sentence_train_norm)\n",
    "        \n",
    "        cross_modal_vec_train = cross_modal_vec_train.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        mid_output = self.fc1(cross_modal_vec_train)\n",
    "        mid_output = F.relu(mid_output)\n",
    "        sim_score_mat_train = self.fc2(mid_output).squeeze(0)\n",
    "        \n",
    "        return sim_score_mat_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TALL_model = TALL()\n",
    "lr = 0.001\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "optimizer = torch.optim.Adam(TALL_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # compute alignment and regression loss\n",
    "    Max_loss = 1000\n",
    "    print(\"start traning.......\")\n",
    "    for epoch in range(EPOCH):\n",
    "        train_loss = 0\n",
    "        loss_align_sum = 0\n",
    "        loss_reg_sum = 0\n",
    "        count = 0\n",
    "        batch_train = batch_iter(sentence_tarin_batch, target_train_csv, target_train_list, BATCH_SIZE)\n",
    "        for x_batch, y_csv, target_time in batch_train:\n",
    "            if y_csv.shape[0]==BATCH_SIZE:\n",
    "                count += 1\n",
    "                x = Variable(torch.LongTensor(x_batch))\n",
    "                y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                offsets = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "                outputs = TALL_model(y, x)\n",
    "                sim_score_mat = outputs[0]\n",
    "                p_reg_mat = outputs[1]\n",
    "                l_reg_mat = outputs[2]\n",
    "                # loss cls, not considering iou\n",
    "                input_size = outputs.size(1)\n",
    "                I = torch.eye(input_size)\n",
    "                I_2 = -2 * I\n",
    "                all1 = torch.ones(input_size, input_size)\n",
    "\n",
    "                mask_mat = I_2 + all1  \n",
    "\n",
    "                alpha = 1.0 / input_size\n",
    "                lambda_regression = 0.01\n",
    "                batch_para_mat = alpha * all1\n",
    "                para_mat = I + batch_para_mat\n",
    "\n",
    "                loss_mat = torch.log(all1 + torch.exp(mask_mat*sim_score_mat))\n",
    "                loss_mat = loss_mat*para_mat\n",
    "                loss_align = loss_mat.mean()\n",
    "\n",
    "                # regression loss\n",
    "                l_reg_diag = torch.mm(l_reg_mat*I, torch.ones(input_size, 1))\n",
    "                p_reg_diag = torch.mm(p_reg_mat*I, torch.ones(input_size, 1))\n",
    "                offset_pred = torch.cat([p_reg_diag, l_reg_diag], 1)\n",
    "                loss_reg = torch.abs(offset_pred - offsets).mean() # L1 loss\n",
    "\n",
    "                loss= lambda_regression*loss_reg +loss_align\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss\n",
    "                loss_reg_sum += loss_reg\n",
    "                loss_align_sum += loss_align\n",
    "        \n",
    "        print('Epoch: %d | Loss: %.3f | loss_align: %.3f | loss_reg: %.3f' % (epoch, train_loss / (count), loss_align_sum / (count), loss_reg_sum / (count)))\n",
    "        train_loss_list.append(train_loss / (count))\n",
    "        \n",
    "        if (epoch + 1)%5 == 0:\n",
    "            print(\"validation.....\")\n",
    "            val_loss = 0\n",
    "            valloss_align_sum = 0\n",
    "            valloss_reg_sum = 0\n",
    "            count = 0\n",
    "            batch_val = batch_iter(sentence_val_batch, target_val_csv, target_val_list, BATCH_SIZE)\n",
    "            for x_batch, y_csv, target_time in batch_val:\n",
    "                if y_csv.shape[0]==BATCH_SIZE:\n",
    "                    count += 1\n",
    "                    x = Variable(torch.LongTensor(x_batch))\n",
    "                    y = Variable(torch.FloatTensor(np.array(y_csv)))\n",
    "                    offsets = Variable(torch.FloatTensor(np.array(target_time)))\n",
    "                    outputs = TALL_model(y, x)\n",
    "                    sim_score_mat = outputs[0]\n",
    "                    p_reg_mat = outputs[1]\n",
    "                    l_reg_mat = outputs[2]\n",
    "                    input_size = outputs.size(1)\n",
    "                    I = torch.eye(input_size)\n",
    "                    I_2 = -2 * I\n",
    "                    all1 = torch.ones(input_size, input_size)\n",
    "\n",
    "                    mask_mat = I_2 + all1  \n",
    "\n",
    "                    alpha = 1.0 / input_size\n",
    "                    lambda_regression = 0.01\n",
    "                    batch_para_mat = alpha * all1\n",
    "                    para_mat = I + batch_para_mat\n",
    "\n",
    "                    loss_mat = torch.log(all1 + torch.exp(mask_mat*sim_score_mat))\n",
    "                    loss_mat = loss_mat*para_mat\n",
    "                    loss_align = loss_mat.mean()\n",
    "\n",
    "                    # regression loss\n",
    "                    l_reg_diag = torch.mm(l_reg_mat*I, torch.ones(input_size, 1))\n",
    "                    p_reg_diag = torch.mm(p_reg_mat*I, torch.ones(input_size, 1))\n",
    "                    offset_pred = torch.cat([p_reg_diag, l_reg_diag], 1)\n",
    "                    loss_reg = torch.abs(offset_pred - offsets).mean() # L1 loss\n",
    "\n",
    "                    loss= lambda_regression*loss_reg +loss_align\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    val_loss += loss\n",
    "                    valloss_reg_sum += loss_reg\n",
    "                    valloss_align_sum += loss_align\n",
    "\n",
    "            print('Epoch: %d | Loss: %.3f | loss_align: %.3f | loss_reg: %.3f' % (epoch, val_loss / (count), valloss_align_sum / (count), valloss_reg_sum / (count)))\n",
    "            val_loss_list.append(val_loss / (count))\n",
    "            if val_loss / count < Max_loss:\n",
    "                torch.save(TALL_model.state_dict(), save_path + 'epoch'+str(epoch)+'params.pkl')\n",
    "                Max_loss = val_loss / count\n",
    "                print(\"model save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traning.......\n",
      "Epoch: 0 | Loss: 1.428 | loss_align: 0.022 | loss_reg: 140.671\n",
      "Epoch: 1 | Loss: 1.391 | loss_align: 0.022 | loss_reg: 136.938\n",
      "Epoch: 2 | Loss: 1.271 | loss_align: 0.022 | loss_reg: 124.962\n",
      "Epoch: 3 | Loss: 1.092 | loss_align: 0.022 | loss_reg: 107.039\n",
      "Epoch: 4 | Loss: 0.911 | loss_align: 0.022 | loss_reg: 88.935\n",
      "validation.....\n",
      "Epoch: 4 | Loss: 0.776 | loss_align: 0.022 | loss_reg: 75.465\n",
      "model save!\n",
      "Epoch: 5 | Loss: 0.789 | loss_align: 0.022 | loss_reg: 76.747\n",
      "Epoch: 6 | Loss: 0.774 | loss_align: 0.022 | loss_reg: 75.212\n",
      "Epoch: 7 | Loss: 0.773 | loss_align: 0.022 | loss_reg: 75.113\n",
      "Epoch: 8 | Loss: 0.774 | loss_align: 0.022 | loss_reg: 75.259\n",
      "Epoch: 9 | Loss: 0.768 | loss_align: 0.022 | loss_reg: 74.659\n",
      "validation.....\n",
      "Epoch: 9 | Loss: 0.709 | loss_align: 0.022 | loss_reg: 68.754\n",
      "model save!\n",
      "Epoch: 10 | Loss: 0.769 | loss_align: 0.022 | loss_reg: 74.742\n",
      "Epoch: 11 | Loss: 0.768 | loss_align: 0.022 | loss_reg: 74.585\n",
      "Epoch: 12 | Loss: 0.771 | loss_align: 0.022 | loss_reg: 74.898\n",
      "Epoch: 13 | Loss: 0.768 | loss_align: 0.022 | loss_reg: 74.596\n",
      "Epoch: 14 | Loss: 0.769 | loss_align: 0.022 | loss_reg: 74.735\n",
      "validation.....\n",
      "Epoch: 14 | Loss: 0.698 | loss_align: 0.022 | loss_reg: 67.662\n",
      "model save!\n",
      "Epoch: 15 | Loss: 0.765 | loss_align: 0.022 | loss_reg: 74.340\n",
      "Epoch: 16 | Loss: 0.763 | loss_align: 0.022 | loss_reg: 74.101\n",
      "Epoch: 17 | Loss: 0.758 | loss_align: 0.022 | loss_reg: 73.624\n",
      "Epoch: 18 | Loss: 0.754 | loss_align: 0.022 | loss_reg: 73.267\n",
      "Epoch: 19 | Loss: 0.747 | loss_align: 0.022 | loss_reg: 72.539\n",
      "validation.....\n",
      "Epoch: 19 | Loss: 0.684 | loss_align: 0.022 | loss_reg: 66.197\n",
      "model save!\n",
      "Epoch: 20 | Loss: 0.736 | loss_align: 0.022 | loss_reg: 71.479\n",
      "Epoch: 21 | Loss: 0.731 | loss_align: 0.022 | loss_reg: 70.978\n",
      "Epoch: 22 | Loss: 0.723 | loss_align: 0.022 | loss_reg: 70.181\n",
      "Epoch: 23 | Loss: 0.718 | loss_align: 0.022 | loss_reg: 69.674\n",
      "Epoch: 24 | Loss: 0.711 | loss_align: 0.022 | loss_reg: 68.971\n",
      "validation.....\n",
      "Epoch: 24 | Loss: 0.672 | loss_align: 0.022 | loss_reg: 65.017\n",
      "model save!\n",
      "Epoch: 25 | Loss: 0.708 | loss_align: 0.022 | loss_reg: 68.679\n",
      "Epoch: 26 | Loss: 0.706 | loss_align: 0.022 | loss_reg: 68.427\n",
      "Epoch: 27 | Loss: 0.703 | loss_align: 0.022 | loss_reg: 68.117\n",
      "Epoch: 28 | Loss: 0.695 | loss_align: 0.022 | loss_reg: 67.298\n",
      "Epoch: 29 | Loss: 0.692 | loss_align: 0.022 | loss_reg: 67.074\n",
      "validation.....\n",
      "Epoch: 29 | Loss: 0.669 | loss_align: 0.022 | loss_reg: 64.697\n",
      "model save!\n",
      "Epoch: 30 | Loss: 0.688 | loss_align: 0.022 | loss_reg: 66.598\n",
      "Epoch: 31 | Loss: 0.684 | loss_align: 0.022 | loss_reg: 66.203\n",
      "Epoch: 32 | Loss: 0.678 | loss_align: 0.022 | loss_reg: 65.597\n",
      "Epoch: 33 | Loss: 0.678 | loss_align: 0.022 | loss_reg: 65.660\n",
      "Epoch: 34 | Loss: 0.674 | loss_align: 0.022 | loss_reg: 65.188\n",
      "validation.....\n",
      "Epoch: 34 | Loss: 0.669 | loss_align: 0.022 | loss_reg: 64.762\n",
      "Epoch: 35 | Loss: 0.669 | loss_align: 0.022 | loss_reg: 64.755\n",
      "Epoch: 36 | Loss: 0.662 | loss_align: 0.022 | loss_reg: 64.072\n",
      "Epoch: 37 | Loss: 0.667 | loss_align: 0.022 | loss_reg: 64.539\n",
      "Epoch: 38 | Loss: 0.659 | loss_align: 0.022 | loss_reg: 63.765\n",
      "Epoch: 39 | Loss: 0.655 | loss_align: 0.022 | loss_reg: 63.376\n",
      "validation.....\n",
      "Epoch: 39 | Loss: 0.681 | loss_align: 0.022 | loss_reg: 65.938\n",
      "Epoch: 40 | Loss: 0.649 | loss_align: 0.022 | loss_reg: 62.703\n",
      "Epoch: 41 | Loss: 0.650 | loss_align: 0.022 | loss_reg: 62.799\n",
      "Epoch: 42 | Loss: 0.643 | loss_align: 0.022 | loss_reg: 62.107\n",
      "Epoch: 43 | Loss: 0.643 | loss_align: 0.022 | loss_reg: 62.131\n",
      "Epoch: 44 | Loss: 0.638 | loss_align: 0.022 | loss_reg: 61.618\n",
      "validation.....\n",
      "Epoch: 44 | Loss: 0.682 | loss_align: 0.022 | loss_reg: 65.994\n",
      "Epoch: 45 | Loss: 0.630 | loss_align: 0.022 | loss_reg: 60.841\n",
      "Epoch: 46 | Loss: 0.629 | loss_align: 0.022 | loss_reg: 60.774\n",
      "Epoch: 47 | Loss: 0.627 | loss_align: 0.022 | loss_reg: 60.557\n",
      "Epoch: 48 | Loss: 0.624 | loss_align: 0.022 | loss_reg: 60.280\n",
      "Epoch: 49 | Loss: 0.619 | loss_align: 0.022 | loss_reg: 59.688\n",
      "validation.....\n",
      "Epoch: 49 | Loss: 0.683 | loss_align: 0.022 | loss_reg: 66.164\n",
      "Epoch: 50 | Loss: 0.615 | loss_align: 0.022 | loss_reg: 59.295\n",
      "Epoch: 51 | Loss: 0.618 | loss_align: 0.022 | loss_reg: 59.681\n",
      "Epoch: 52 | Loss: 0.604 | loss_align: 0.022 | loss_reg: 58.239\n",
      "Epoch: 53 | Loss: 0.605 | loss_align: 0.022 | loss_reg: 58.348\n",
      "Epoch: 54 | Loss: 0.607 | loss_align: 0.022 | loss_reg: 58.490\n",
      "validation.....\n",
      "Epoch: 54 | Loss: 0.687 | loss_align: 0.022 | loss_reg: 66.501\n",
      "Epoch: 55 | Loss: 0.601 | loss_align: 0.022 | loss_reg: 57.898\n",
      "Epoch: 56 | Loss: 0.597 | loss_align: 0.022 | loss_reg: 57.567\n",
      "Epoch: 57 | Loss: 0.591 | loss_align: 0.022 | loss_reg: 56.906\n",
      "Epoch: 58 | Loss: 0.590 | loss_align: 0.022 | loss_reg: 56.790\n",
      "Epoch: 59 | Loss: 0.587 | loss_align: 0.022 | loss_reg: 56.534\n",
      "validation.....\n",
      "Epoch: 59 | Loss: 0.686 | loss_align: 0.022 | loss_reg: 66.392\n",
      "Epoch: 60 | Loss: 0.589 | loss_align: 0.022 | loss_reg: 56.762\n",
      "Epoch: 61 | Loss: 0.586 | loss_align: 0.022 | loss_reg: 56.463\n",
      "Epoch: 62 | Loss: 0.578 | loss_align: 0.022 | loss_reg: 55.676\n",
      "Epoch: 63 | Loss: 0.574 | loss_align: 0.022 | loss_reg: 55.238\n",
      "Epoch: 64 | Loss: 0.575 | loss_align: 0.022 | loss_reg: 55.331\n",
      "validation.....\n",
      "Epoch: 64 | Loss: 0.688 | loss_align: 0.022 | loss_reg: 66.624\n",
      "Epoch: 65 | Loss: 0.568 | loss_align: 0.022 | loss_reg: 54.610\n",
      "Epoch: 66 | Loss: 0.569 | loss_align: 0.022 | loss_reg: 54.729\n",
      "Epoch: 67 | Loss: 0.563 | loss_align: 0.022 | loss_reg: 54.090\n",
      "Epoch: 68 | Loss: 0.562 | loss_align: 0.022 | loss_reg: 53.986\n",
      "Epoch: 69 | Loss: 0.558 | loss_align: 0.022 | loss_reg: 53.653\n",
      "validation.....\n",
      "Epoch: 69 | Loss: 0.691 | loss_align: 0.022 | loss_reg: 66.917\n",
      "Epoch: 70 | Loss: 0.555 | loss_align: 0.022 | loss_reg: 53.381\n",
      "Epoch: 71 | Loss: 0.549 | loss_align: 0.022 | loss_reg: 52.707\n",
      "Epoch: 72 | Loss: 0.552 | loss_align: 0.022 | loss_reg: 53.068\n",
      "Epoch: 73 | Loss: 0.552 | loss_align: 0.022 | loss_reg: 53.014\n",
      "Epoch: 74 | Loss: 0.546 | loss_align: 0.022 | loss_reg: 52.421\n",
      "validation.....\n",
      "Epoch: 74 | Loss: 0.685 | loss_align: 0.022 | loss_reg: 66.334\n",
      "Epoch: 75 | Loss: 0.544 | loss_align: 0.022 | loss_reg: 52.233\n",
      "Epoch: 76 | Loss: 0.539 | loss_align: 0.022 | loss_reg: 51.706\n",
      "Epoch: 77 | Loss: 0.537 | loss_align: 0.022 | loss_reg: 51.584\n",
      "Epoch: 78 | Loss: 0.530 | loss_align: 0.022 | loss_reg: 50.880\n",
      "Epoch: 79 | Loss: 0.529 | loss_align: 0.022 | loss_reg: 50.694\n",
      "validation.....\n",
      "Epoch: 79 | Loss: 0.691 | loss_align: 0.022 | loss_reg: 66.924\n",
      "Epoch: 80 | Loss: 0.527 | loss_align: 0.022 | loss_reg: 50.502\n",
      "Epoch: 81 | Loss: 0.527 | loss_align: 0.022 | loss_reg: 50.544\n",
      "Epoch: 82 | Loss: 0.521 | loss_align: 0.022 | loss_reg: 49.956\n",
      "Epoch: 83 | Loss: 0.518 | loss_align: 0.022 | loss_reg: 49.617\n",
      "Epoch: 84 | Loss: 0.523 | loss_align: 0.022 | loss_reg: 50.103\n",
      "validation.....\n",
      "Epoch: 84 | Loss: 0.684 | loss_align: 0.022 | loss_reg: 66.254\n",
      "Epoch: 85 | Loss: 0.516 | loss_align: 0.022 | loss_reg: 49.389\n",
      "Epoch: 86 | Loss: 0.511 | loss_align: 0.022 | loss_reg: 48.970\n",
      "Epoch: 87 | Loss: 0.511 | loss_align: 0.022 | loss_reg: 48.896\n",
      "Epoch: 88 | Loss: 0.506 | loss_align: 0.022 | loss_reg: 48.477\n",
      "Epoch: 89 | Loss: 0.502 | loss_align: 0.022 | loss_reg: 47.992\n",
      "validation.....\n",
      "Epoch: 89 | Loss: 0.690 | loss_align: 0.022 | loss_reg: 66.809\n",
      "Epoch: 90 | Loss: 0.501 | loss_align: 0.022 | loss_reg: 47.901\n",
      "Epoch: 91 | Loss: 0.493 | loss_align: 0.022 | loss_reg: 47.120\n",
      "Epoch: 92 | Loss: 0.496 | loss_align: 0.022 | loss_reg: 47.395\n",
      "Epoch: 93 | Loss: 0.489 | loss_align: 0.022 | loss_reg: 46.779\n",
      "Epoch: 94 | Loss: 0.493 | loss_align: 0.022 | loss_reg: 47.085\n",
      "validation.....\n",
      "Epoch: 94 | Loss: 0.687 | loss_align: 0.022 | loss_reg: 66.554\n",
      "Epoch: 95 | Loss: 0.488 | loss_align: 0.022 | loss_reg: 46.656\n",
      "Epoch: 96 | Loss: 0.489 | loss_align: 0.022 | loss_reg: 46.753\n",
      "Epoch: 97 | Loss: 0.484 | loss_align: 0.022 | loss_reg: 46.279\n",
      "Epoch: 98 | Loss: 0.477 | loss_align: 0.022 | loss_reg: 45.575\n",
      "Epoch: 99 | Loss: 0.481 | loss_align: 0.022 | loss_reg: 45.898\n",
      "validation.....\n",
      "Epoch: 99 | Loss: 0.689 | loss_align: 0.022 | loss_reg: 66.728\n",
      "Epoch: 100 | Loss: 0.485 | loss_align: 0.022 | loss_reg: 46.353\n",
      "Epoch: 101 | Loss: 0.478 | loss_align: 0.022 | loss_reg: 45.667\n",
      "Epoch: 102 | Loss: 0.472 | loss_align: 0.022 | loss_reg: 45.074\n",
      "Epoch: 103 | Loss: 0.465 | loss_align: 0.022 | loss_reg: 44.291\n",
      "Epoch: 104 | Loss: 0.463 | loss_align: 0.022 | loss_reg: 44.170\n",
      "validation.....\n",
      "Epoch: 104 | Loss: 0.690 | loss_align: 0.022 | loss_reg: 66.791\n",
      "Epoch: 105 | Loss: 0.461 | loss_align: 0.022 | loss_reg: 43.976\n",
      "Epoch: 106 | Loss: 0.461 | loss_align: 0.022 | loss_reg: 43.971\n",
      "Epoch: 107 | Loss: 0.466 | loss_align: 0.022 | loss_reg: 44.420\n",
      "Epoch: 108 | Loss: 0.457 | loss_align: 0.022 | loss_reg: 43.529\n",
      "Epoch: 109 | Loss: 0.457 | loss_align: 0.022 | loss_reg: 43.545\n",
      "validation.....\n",
      "Epoch: 109 | Loss: 0.686 | loss_align: 0.022 | loss_reg: 66.449\n",
      "Epoch: 110 | Loss: 0.450 | loss_align: 0.022 | loss_reg: 42.804\n",
      "Epoch: 111 | Loss: 0.449 | loss_align: 0.022 | loss_reg: 42.770\n",
      "Epoch: 112 | Loss: 0.452 | loss_align: 0.022 | loss_reg: 43.063\n",
      "Epoch: 113 | Loss: 0.443 | loss_align: 0.022 | loss_reg: 42.177\n",
      "Epoch: 114 | Loss: 0.441 | loss_align: 0.022 | loss_reg: 41.950\n",
      "validation.....\n",
      "Epoch: 114 | Loss: 0.684 | loss_align: 0.022 | loss_reg: 66.268\n",
      "Epoch: 115 | Loss: 0.442 | loss_align: 0.022 | loss_reg: 41.989\n",
      "Epoch: 116 | Loss: 0.438 | loss_align: 0.022 | loss_reg: 41.651\n",
      "Epoch: 117 | Loss: 0.434 | loss_align: 0.022 | loss_reg: 41.244\n",
      "Epoch: 118 | Loss: 0.436 | loss_align: 0.022 | loss_reg: 41.390\n",
      "Epoch: 119 | Loss: 0.430 | loss_align: 0.022 | loss_reg: 40.835\n",
      "validation.....\n",
      "Epoch: 119 | Loss: 0.680 | loss_align: 0.022 | loss_reg: 65.816\n",
      "Epoch: 120 | Loss: 0.431 | loss_align: 0.022 | loss_reg: 40.958\n",
      "Epoch: 121 | Loss: 0.428 | loss_align: 0.022 | loss_reg: 40.621\n",
      "Epoch: 122 | Loss: 0.424 | loss_align: 0.022 | loss_reg: 40.205\n",
      "Epoch: 123 | Loss: 0.422 | loss_align: 0.022 | loss_reg: 40.028\n",
      "Epoch: 124 | Loss: 0.420 | loss_align: 0.022 | loss_reg: 39.823\n",
      "validation.....\n",
      "Epoch: 124 | Loss: 0.669 | loss_align: 0.022 | loss_reg: 64.749\n",
      "Epoch: 125 | Loss: 0.429 | loss_align: 0.022 | loss_reg: 40.729\n",
      "Epoch: 126 | Loss: 0.421 | loss_align: 0.022 | loss_reg: 39.912\n",
      "Epoch: 127 | Loss: 0.418 | loss_align: 0.022 | loss_reg: 39.674\n",
      "Epoch: 128 | Loss: 0.413 | loss_align: 0.022 | loss_reg: 39.091\n",
      "Epoch: 129 | Loss: 0.410 | loss_align: 0.022 | loss_reg: 38.829\n",
      "validation.....\n",
      "Epoch: 129 | Loss: 0.671 | loss_align: 0.022 | loss_reg: 64.890\n",
      "Epoch: 130 | Loss: 0.422 | loss_align: 0.022 | loss_reg: 40.071\n",
      "Epoch: 131 | Loss: 0.412 | loss_align: 0.022 | loss_reg: 39.036\n",
      "Epoch: 132 | Loss: 0.406 | loss_align: 0.022 | loss_reg: 38.462\n",
      "Epoch: 133 | Loss: 0.411 | loss_align: 0.022 | loss_reg: 38.885\n",
      "Epoch: 134 | Loss: 0.400 | loss_align: 0.022 | loss_reg: 37.873\n",
      "validation.....\n",
      "Epoch: 134 | Loss: 0.671 | loss_align: 0.022 | loss_reg: 64.952\n",
      "Epoch: 135 | Loss: 0.412 | loss_align: 0.022 | loss_reg: 39.025\n",
      "Epoch: 136 | Loss: 0.401 | loss_align: 0.022 | loss_reg: 37.977\n",
      "Epoch: 137 | Loss: 0.395 | loss_align: 0.022 | loss_reg: 37.304\n",
      "Epoch: 138 | Loss: 0.393 | loss_align: 0.022 | loss_reg: 37.128\n",
      "Epoch: 139 | Loss: 0.391 | loss_align: 0.022 | loss_reg: 36.952\n",
      "validation.....\n",
      "Epoch: 139 | Loss: 0.665 | loss_align: 0.022 | loss_reg: 64.377\n",
      "model save!\n",
      "Epoch: 140 | Loss: 0.392 | loss_align: 0.022 | loss_reg: 37.001\n",
      "Epoch: 141 | Loss: 0.384 | loss_align: 0.022 | loss_reg: 36.200\n",
      "Epoch: 142 | Loss: 0.386 | loss_align: 0.022 | loss_reg: 36.406\n",
      "Epoch: 143 | Loss: 0.388 | loss_align: 0.022 | loss_reg: 36.594\n",
      "Epoch: 144 | Loss: 0.381 | loss_align: 0.022 | loss_reg: 35.949\n",
      "validation.....\n",
      "Epoch: 144 | Loss: 0.676 | loss_align: 0.022 | loss_reg: 65.386\n",
      "Epoch: 145 | Loss: 0.379 | loss_align: 0.022 | loss_reg: 35.772\n",
      "Epoch: 146 | Loss: 0.382 | loss_align: 0.022 | loss_reg: 36.013\n",
      "Epoch: 147 | Loss: 0.376 | loss_align: 0.022 | loss_reg: 35.454\n",
      "Epoch: 148 | Loss: 0.373 | loss_align: 0.022 | loss_reg: 35.180\n",
      "Epoch: 149 | Loss: 0.378 | loss_align: 0.022 | loss_reg: 35.655\n",
      "validation.....\n",
      "Epoch: 149 | Loss: 0.677 | loss_align: 0.022 | loss_reg: 65.552\n",
      "Epoch: 150 | Loss: 0.373 | loss_align: 0.022 | loss_reg: 35.112\n",
      "Epoch: 151 | Loss: 0.382 | loss_align: 0.022 | loss_reg: 36.021\n",
      "Epoch: 152 | Loss: 0.376 | loss_align: 0.022 | loss_reg: 35.390\n",
      "Epoch: 153 | Loss: 0.368 | loss_align: 0.022 | loss_reg: 34.638\n",
      "Epoch: 154 | Loss: 0.384 | loss_align: 0.022 | loss_reg: 36.215\n",
      "validation.....\n",
      "Epoch: 154 | Loss: 0.671 | loss_align: 0.022 | loss_reg: 64.935\n",
      "Epoch: 155 | Loss: 0.369 | loss_align: 0.022 | loss_reg: 34.731\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-1a32f5328526>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlambda_regression\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_reg\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mloss_align\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw(train_loss_list, val_loss_list):\n",
    "    x1 = range(0, len(train_loss_list))\n",
    "    x2 = range(0, len(val_loss_list))\n",
    "    #with plt.style.context(['science']):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x1, train_loss_list[:len(train_loss_list)], 'o-')\n",
    "    plt.title('train loss vs. epoches')\n",
    "    plt.ylabel('train loss')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x2,val_loss_list[:len(val_loss_list)] , '.-')\n",
    "    plt.xlabel('Val loss vs. epoches')\n",
    "    plt.ylabel('Val loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(train_loss_list, val_loss_list)\n",
    "torch.save(TALL_model.state_dict(), save_path + 'final_model_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class LSTM_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(5000, 64)\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        #self.rnn = nn.GRU(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        self.f1 = nn.Sequential(nn.Linear(256,128),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.ReLU())\n",
    "\n",
    "        self.f2 = nn.Sequential(nn.Linear(128,64))\n",
    "        \n",
    "        \n",
    "        self.conv1=torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1,10,3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(2),\n",
    "        )\n",
    "        \n",
    "        self.conv2=torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(10,20,3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(2),\n",
    "        )\n",
    "\n",
    "        #self.fc1=torch.nn.Linear(2520,128)\n",
    "        self.fc1=torch.nn.Linear(512,128)\n",
    "        self.fc1_drop=torch.nn.Dropout(p=0.4)\n",
    "        self.fc2=torch.nn.Linear(128, 64)\n",
    "        \n",
    "        #特征融合\n",
    "        self.final_fc = nn.Linear(in_features=128, out_features=64)\n",
    "        self.score_fc = torch.nn.Conv2d(64,3,kernel_size=1,stride=1)\n",
    "        \n",
    "        \n",
    "    def cnnout(self, x2):\n",
    "        in_fc=x2.view(x2.size(0),-1)\n",
    "        out_fc1=self.fc1(in_fc)\n",
    "        out_drop=self.fc1_drop(out_fc1)\n",
    "        out_fc2=self.fc2(out_drop)\n",
    "        return out_fc2\n",
    "        \n",
    "    def forward(self, x1, x2): \n",
    "        if x1.shape[0]!=2:\n",
    "            #lstm\n",
    "            x = self.embedding(x1)\n",
    "            x,_ = self.rnn(x)\n",
    "            x = F.dropout(x,p=0.8)\n",
    "            \n",
    "            x = self.f1(x[:,-1,:])\n",
    "            lstm_output = self.f2(x)\n",
    "            cnn_out=self.cnnout(x2)\n",
    "            #concat\n",
    "            output = torch.cat((lstm_output, cnn_out), 1)\n",
    "            output = self.final_fc(output)\n",
    "            return output\n",
    "        else:\n",
    "            cnn_out=self.cnnout(x2)\n",
    "            return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class Change(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Change, self).__init__()\n",
    "        #特征融合\n",
    "        self.score_fc = torch.nn.Conv1d(64, 2, kernel_size=1, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = x#[10, 64]\n",
    "        output = output.unsqueeze(2)\n",
    "        score = self.score_fc(output)\n",
    "        offset_pred = score.squeeze(2)\n",
    "        return offset_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TALL_final_model = TALL()\n",
    "My_model = Change()\n",
    "middle_model = LSTM_CNN()\n",
    "TALL_final_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/TALL/save_model/final_model_params.pkl'))\n",
    "middle_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model/189epoch_20200713_64_dim_params.pkl'))\n",
    "My_model.load_state_dict(torch.load('C:/Users/wuxun/Desktop/Data/save_model2/epoch257_batchsize1_params.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固定模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TALL(\n",
       "  (v2s_lt): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  (s2s_lt): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc2): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (embedding): Embedding(5000, 64)\n",
       "  (lstm): LSTM(64, 128, num_layers=2, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "My_model.eval()\n",
    "middle_model.eval()\n",
    "TALL_final_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IoU_thresh = [0.1, 0.3, 0.5, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_IoU(i0, i1):\n",
    "    # calculate temporal intersection over union\n",
    "    union = (min(i0[0], i1[0]), max(i0[1], i1[1]))\n",
    "    inter = (max(i0[0], i1[0]), min(i0[1], i1[1]))\n",
    "    iou = 1.0*(inter[1]-inter[0])/(union[1]-union[0])\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_IoU_recall_top_n_forreg(iou_thresh, time_mat, time_pre_mat):#top\n",
    "    \n",
    "    \n",
    "    \n",
    "    iou = calculate_IoU((gt_start, gt_end),(pred_start, pred_end))\n",
    "    if iou>=iou_thresh:\n",
    "        correct_num+=1\n",
    "    return correct_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_for_TALL(outputs):\n",
    "    p_reg_mat = outputs[1]\n",
    "    l_reg_mat = outputs[2]\n",
    "    # regression loss\n",
    "    l_reg_diag = torch.mm(l_reg_mat*I, torch.ones(input_size, 1))\n",
    "    p_reg_diag = torch.mm(p_reg_mat*I, torch.ones(input_size, 1))\n",
    "    offset_pred = torch.cat([p_reg_diag, l_reg_diag], 1)\n",
    "    return offset_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取test数据集\n",
    "def get_test_dict_for_TALL(path, csv_path):\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    data_id = []\n",
    "    csv=[]\n",
    "    time_list=[]\n",
    "    Max_len=-1\n",
    "    count=0\n",
    "    with open(path) as contents:\n",
    "        for line in contents:\n",
    "            count+=1\n",
    "            List = line.split('#')\n",
    "            video_name = List[0]\n",
    "            time_length = float(List[1])\n",
    "            foldtype = List[2]\n",
    "            recipetype = List[3]\n",
    "            target = List[4]\n",
    "            \n",
    "            #将句子转换为id表示：\n",
    "            sentence = List[5].strip('\\n').strip()\n",
    "            sentence = re.split(r\"[,| |.]\",sentence)\n",
    "            sentence_id = [word_to_id[x] for x in sentence if x in word_to_id]\n",
    "            if len(sentence_id) > Max_len:\n",
    "                Max_len = len(sentence_id)\n",
    "            data_id.append(sentence_id)\n",
    "            \n",
    "            #寻找路径,先统一取0001\n",
    "            dir_path = csv_path+'/'+foldtype+'/'+recipetype+'/'+video_name+'/0001/'\n",
    "            name = os.listdir(dir_path)[0]\n",
    "            dir_path = dir_path + name\n",
    "            \n",
    "            #读取csv文件\n",
    "            my_file = Path(dir_path)\n",
    "            if my_file.exists():\n",
    "                frame_sum = pd.read_csv(dir_path, header=None)\n",
    "            else:\n",
    "                print(\"目录不存在！\")\n",
    "            \n",
    "            #确定时间点，前帧后帧取pooling\n",
    "            target = target.split('_')\n",
    "            cur_start = float(target[0])\n",
    "            cur_end = float(target[1])\n",
    "            middle_time = (cur_start + cur_end)//2\n",
    "            \n",
    "            #中间帧\n",
    "            target_frame_num = int(middle_time/time_length*500)\n",
    "            target_middle_frame = frame_sum.loc[target_frame_num]\n",
    "            \n",
    "            #上下文信息\n",
    "            target_frame_start = int(cur_start/time_length*500)\n",
    "            target_frame_end = int(cur_end/time_length*500)\n",
    "            if target_frame_start == target_frame_num:\n",
    "                print(str(cur_start)+\"  \"+str(cur_end)+\" \"+str(time_length))\n",
    "                print(\"出现重复！\")\n",
    "                target_frame_start = min(target_frame_num - 3, 0)\n",
    "            if  target_frame_end ==target_frame_num:\n",
    "                print(str(cur_start)+\"  \"+str(cur_end)+\" \"+str(time_length))\n",
    "                print(\"出现重复！\")\n",
    "                target_frame_start = min(target_frame_num + 3, 499)\n",
    "                \n",
    "            pre_context = np.zeros([target_frame_num - target_frame_start, 512], dtype=np.float32)\n",
    "            post_context = np.zeros([target_frame_end - target_frame_num, 512], dtype=np.float32) \n",
    "            for i in range(target_frame_num - target_frame_start):\n",
    "                pre_context[i] = frame_sum.loc[i]\n",
    "                \n",
    "            for i in range(target_frame_end - target_frame_num):\n",
    "                post_context[i] = frame_sum.loc[i]\n",
    "            \n",
    "            #对pre_context和post_context取均值\n",
    "            pre_context = np.mean(pre_context, axis=0)\n",
    "            post_context = np.mean(post_context, axis=0)\n",
    "            \n",
    "            #对三段信息进行拼接\n",
    "            image = np.hstack((pre_context, target_middle_frame, post_context))\n",
    "            \n",
    "            csv.append(image)\n",
    "            time_list.append([cur_start, cur_end])\n",
    "            \n",
    "    #将所有的句子pad为同一最大长度\n",
    "    batch_data_id = np.array([line +[0]*(Max_len-len(line)) \n",
    "                                            for line in data_id])\n",
    "    batch_seq = torch.LongTensor(batch_data_id)    \n",
    "    print(len(batch_seq),len(csv),len(time_list))\n",
    "    \n",
    "    return batch_seq, csv, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取test集\n",
    "# 读取test数据集\n",
    "def get_test_dict_for_My_model(path, csv_path):\n",
    "    words, word_to_id = read_vocab(vocab_dir)\n",
    "    data_id = []\n",
    "    csv=[]\n",
    "    time_list=[]\n",
    "    Max_len=-1\n",
    "    count=0\n",
    "    with open(path) as contents:\n",
    "        for line in contents:\n",
    "            count+=1\n",
    "            List = line.split('#')\n",
    "            video_name = List[0]\n",
    "            time_length = float(List[1])\n",
    "            foldtype = List[2]\n",
    "            recipetype = List[3]\n",
    "            target = List[4]\n",
    "            \n",
    "            #将句子转换为id表示：\n",
    "            sentence = List[5].strip('\\n').strip()\n",
    "            sentence = re.split(r\"[,| |.]\",sentence)\n",
    "            sentence_id = [word_to_id[x] for x in sentence if x in word_to_id]\n",
    "            if len(sentence_id) > Max_len:\n",
    "                Max_len = len(sentence_id)\n",
    "            data_id.append(sentence_id)\n",
    "            \n",
    "            #寻找路径,先统一取0001\n",
    "            dir_path = csv_path+'/'+foldtype+'/'+recipetype+'/'+video_name+'/0001/'\n",
    "            name = os.listdir(dir_path)[0]\n",
    "            dir_path = dir_path + name\n",
    "            \n",
    "            #读取csv文件\n",
    "            my_file = Path(dir_path)\n",
    "            if my_file.exists():\n",
    "                frame_sum = pd.read_csv(dir_path, header=None)\n",
    "            else:\n",
    "                print(\"目录不存在！\")\n",
    "            \n",
    "            #确定时间点，前帧后帧取pooling\n",
    "            target = target.split('_')\n",
    "            cur_start = float(target[0])\n",
    "            cur_end = float(target[1])\n",
    "            middle_time = (cur_start + cur_end)//2\n",
    "            \n",
    "            #中间帧\n",
    "            target_frame_num = int(middle_time/time_length*500)\n",
    "            target_middle_frame = frame_sum.loc[target_frame_num]\n",
    "            \n",
    "            csv.append(target_middle_frame)\n",
    "            time_list.append([cur_start, cur_end])\n",
    "            \n",
    "    #将所有的句子pad为同一最大长度\n",
    "    batch_data_id = np.array([line +[0]*(Max_len-len(line)) \n",
    "                                            for line in data_id])\n",
    "    batch_seq = torch.LongTensor(batch_data_id)    \n",
    "    print(len(batch_seq),len(csv),len(time_list))\n",
    "    \n",
    "    return batch_seq, csv, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 读取test集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894.0  896.0 1106.12\n",
      "出现重复！\n",
      "132 132 132\n"
     ]
    }
   ],
   "source": [
    "TALL_test_seq, TALL_test_csv, TALL_test_time_list = get_test_dict_for_TALL(test_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 132 132\n"
     ]
    }
   ],
   "source": [
    "My_test_seq, My_test_csv, My_test_time_list = get_test_dict_for_My_model(test_path, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3, 157,   4,  ...,   0,   0,   0],\n",
      "        [ 45,  14,   5,  ...,   0,   0,   0],\n",
      "        [  3,  15, 112,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [ 98, 230,   8,  ...,   0,   0,   0],\n",
      "        [ 28,   2,  14,  ...,   0,   0,   0],\n",
      "        [ 52,   2,  14,  ...,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
